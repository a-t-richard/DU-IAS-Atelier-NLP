{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3",
      "metadata": {
        "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3"
      },
      "source": [
        "# DU IA et Santé - Atelier NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249e0319-7ea9-4c04-855f-8cdc547c134b",
      "metadata": {
        "id": "249e0319-7ea9-4c04-855f-8cdc547c134b"
      },
      "source": [
        "Dans cet atelier, nous allons voir plusieurs techniques de NLP \"moderne\", principalement basées sur les transformers et la librairie [huggingface](https://huggingface.co/).\n",
        "\n",
        "Cet atelier est diviser en trois partie.\n",
        "\n",
        "Dans la partie 1, nous allons nous intéresser à la partie encodage des transformers (BERT et cie.) afin de mieux comprendre les représentations internes de ceux-ci.\n",
        "\n",
        "Dans la partie 2, nous interesserons à la partie decodage des transformers (GPT et cie.) et aux méthodes de *prompt engineering* permettant d’améliorer leurs résultats.\n",
        "\n",
        "Enfin, dans la partie 3, nous allons voir comment méler les deux approches pour construire des assistants personnels.\n",
        "\n",
        "Mais, avant toute choses, il nous faut installer quelques librairies.\n",
        "\n",
        "Pour cela, executez la cellule suivante:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate jupyter-scatter"
      ],
      "metadata": {
        "id": "mVi_HlGyzznr"
      },
      "id": "mVi_HlGyzznr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis, nous allons importer quelques librairies et vérifier que nous accédons bien au GPU (si vous en avez un).\n",
        "\n",
        "Pour cela exécutez la cellule ci-dessous."
      ],
      "metadata": {
        "id": "RLu0vhmeu8Tm"
      },
      "id": "RLu0vhmeu8Tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0",
      "metadata": {
        "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0"
      },
      "outputs": [],
      "source": [
        "# Librairies utilitaires\n",
        "from tqdm.auto import tqdm\n",
        "import ipywidgets\n",
        "import gc\n",
        "\n",
        "# Librairies mathématiques\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jscatter\n",
        "\n",
        "# Librairies ML\n",
        "import torch\n",
        "print(\"cuda available?\", str(torch.cuda.is_available()))\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  torch.cuda.set_device(device)\n",
        "  print(\"cuda version:\", torch.version.cuda)\n",
        "  print(\"cuDNN enabled?\", torch.backends.cudnn.enabled)\n",
        "  print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
        "  print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "# Initialisation générateurs de nombres aléatoires\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.random.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916",
      "metadata": {
        "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916"
      },
      "source": [
        "Si le résultat est : `cuda available? False`, cela signifie que la librairie *torch* n’a pas pu accéder à votre GPU.\n",
        "\n",
        "Dans le cas contraire (ou si vous n’avez pas de GPU à disposition), nous pouvons passer à la suite.\n",
        "\n",
        "remarque: si vous utiliser ce GoogleCollab, cliquez sur Exécution > Modifier le type d’exécution et séléctionnnez l’option T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e",
      "metadata": {
        "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e"
      },
      "source": [
        "## Partie 1 - Encodage et représentations internes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8266d061-7137-4370-b8cd-32a6b6023657",
      "metadata": {
        "id": "8266d061-7137-4370-b8cd-32a6b6023657"
      },
      "source": [
        "Dans cette partie, nous allons voir comment les transformers, plus spécifiquement les modèles de type BERT, encodent et transforment les textes fournis pour les donner en entrée des modèles. Nous allons aussi étudier les représentations internes de ce modèles, leurs espaces latents, et comment manipuler ces réprésentations.\n",
        "\n",
        "Pour cela, nous allons nous baser sur le modèle [CamemBERT](https://camembert-model.fr/), entrainé sur des textes en français.\n",
        "\n",
        "Pour essayer un autre modèle, voir: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55",
      "metadata": {
        "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"Lajavaness/sentence-camembert-base\"\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name, device_map=device, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ded393-8743-4ab4-994d-93b5221c0898",
      "metadata": {
        "id": "91ded393-8743-4ab4-994d-93b5221c0898"
      },
      "source": [
        "### 1.1 Tokenization et encodage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735",
      "metadata": {
        "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735"
      },
      "source": [
        "La première étape pour permettre à un modèle de language de traiter du texte est la tokenisation.\n",
        "\n",
        "Cette étape permet de découper une phrase ou un mot en un ou plusieurs *token* connus du modèle (l’ensemble des tokens connus par un modèle est appelé son \"vocabulaire\").\n",
        "\n",
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986a470a-ee07-49b0-901f-5bdd184df2ea",
      "metadata": {
        "id": "986a470a-ee07-49b0-901f-5bdd184df2ea"
      },
      "outputs": [],
      "source": [
        "sentence = \"fromage\"\n",
        "tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "tokenized_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8238822f-40b5-4402-90ce-80fa206804e7",
      "metadata": {
        "id": "8238822f-40b5-4402-90ce-80fa206804e7"
      },
      "source": [
        "Cependant, un modèle attend en entrée non pas une liste de tokens, mais la liste des id correspondant à ces tokens dans son vocabulaire.\n",
        "\n",
        "Ainsi, il nous faut encoder les tokens obtenus précédemment en une liste d’entier utilisable par le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16",
      "metadata": {
        "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16"
      },
      "outputs": [],
      "source": [
        "encoded_sentence = tokenizer.encode(tokenized_sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
        "encoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b",
      "metadata": {
        "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b"
      },
      "source": [
        "Le résultat ci-dessus présente la liste des ids correspondant aux tokens de notre phrase.\n",
        "\n",
        "Nous pouvons cependant noter la précense de deux ids supplémentaires (5 et 6), ceux-ci correspondants aux tokens de début et fin de phrase.\n",
        "\n",
        "Sentez-vous libre de modifier la phrase d’exemple avant de passer à la suite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5844028a-a654-493d-b718-4548f96e6d93",
      "metadata": {
        "id": "5844028a-a654-493d-b718-4548f96e6d93"
      },
      "source": [
        "### 1.2 Word Embeddings et espaces latents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327",
      "metadata": {
        "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327"
      },
      "source": [
        "Maintenant que nous avons comment tokeniser et encoder un texte pour permettre à un modèle de manipuler ce texte, nous allons voir les réprentations internes utilisé par ces modèles pour manipuler ces textes.\n",
        "\n",
        "Pour cela nous allons utiliser la classe *pipeline* de la librairie *transformers*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "29f66090-2173-4de3-aa30-96204336c294",
      "metadata": {
        "id": "29f66090-2173-4de3-aa30-96204336c294"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8678d1-a6ec-4986-8952-81d3047ac812",
      "metadata": {
        "id": "8f8678d1-a6ec-4986-8952-81d3047ac812"
      },
      "source": [
        "Cette classe permet d’utiliser nos modèles sur divers problèmes.\n",
        "\n",
        "Ici, nous allons l’utiliser pour effectuer de la *feature-extraction* et ainsi obtenir les représentations interne des phrases fournies à notre modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "sSe6E8H-zDpe",
      "metadata": {
        "id": "sSe6E8H-zDpe"
      },
      "outputs": [],
      "source": [
        "pipeline = pipeline('feature-extraction', model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b",
      "metadata": {
        "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b"
      },
      "source": [
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BTW_AQGpLPFU",
      "metadata": {
        "id": "BTW_AQGpLPFU"
      },
      "outputs": [],
      "source": [
        "data = pipeline(sentence)\n",
        "pd.DataFrame(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame([np.array(data[0]).mean(axis=0)])"
      ],
      "metadata": {
        "id": "XVCNmpN7zAX4"
      },
      "id": "XVCNmpN7zAX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6142e726-6475-4eaa-abf3-3319754168b0",
      "metadata": {
        "id": "6142e726-6475-4eaa-abf3-3319754168b0"
      },
      "source": [
        "Nous voyons ici, que notre modèle transforme chaque token de notre phrase en vecteurs de taille:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a",
      "metadata": {
        "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a"
      },
      "outputs": [],
      "source": [
        "len(data[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10a34bf-912a-4042-b18a-af2e1993e220",
      "metadata": {
        "id": "f10a34bf-912a-4042-b18a-af2e1993e220"
      },
      "source": [
        "La taille de ces vecteurs, générallement appellés *embeddings*, determine le nombre de dimensions de l’espace latent de notre modèle.\n",
        "\n",
        "Chaque vecteur correspondant alors à un point dans cet espace.\n",
        "\n",
        "Il est alors possible d’avoir un aperçu du vocabulaire d’un modèle dans son espace latent.\n",
        "\n",
        "Pour cela, il nous faut dans un premier temps recupérer les embeddings de chaque token présent dans le vocabulaire d’un modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QflyI3Rw16Jz",
      "metadata": {
        "id": "QflyI3Rw16Jz"
      },
      "outputs": [],
      "source": [
        "# On récupère le vocabulaire du modèle\n",
        "vocab = [token.replace(\"▁\", \"\") for token in list(tokenizer.get_vocab().keys())]\n",
        "\n",
        "# On encodage le vocabulaire\n",
        "embedded_vocab = pipeline(vocab, batch_size=32)\n",
        "\n",
        "# On met le résultat sous un format plus facile à utiliser\n",
        "df_embedded_vocab = pd.DataFrame([np.array(ev[0]).mean(axis=0) for ev in embedded_vocab], index=vocab)\n",
        "\n",
        "# On affiche le résultat\n",
        "df_embedded_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut alors récupérer l’embedding d’un token specifique, par exemple:"
      ],
      "metadata": {
        "id": "pvca93cDl0jO"
      },
      "id": "pvca93cDl0jO"
    },
    {
      "cell_type": "code",
      "source": [
        "df_embedded_vocab.loc[[sentence]]"
      ],
      "metadata": {
        "id": "2lgFd9VztyU0"
      },
      "id": "2lgFd9VztyU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "FUgDe0parTUw"
      },
      "id": "FUgDe0parTUw",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca_embedded_vocab = pca.fit_transform(df_embedded_vocab)"
      ],
      "metadata": {
        "id": "4YmkUiEFvz8I"
      },
      "id": "4YmkUiEFvz8I",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pca = pd.DataFrame(data = pca_embedded_vocab, columns = ['PC1', 'PC2'], index=vocab)"
      ],
      "metadata": {
        "id": "hRLstL5wwNNR"
      },
      "id": "hRLstL5wwNNR",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scatter = jscatter.Scatter(data=df_pca, x=\"PC1\", y=\"PC2\")\n",
        "\n",
        "output = ipywidgets.Output()\n",
        "\n",
        "@output.capture(clear_output=True)\n",
        "def selection_change_handler(change):\n",
        "  display(df_pca.iloc[change.new])\n",
        "\n",
        "scatter.widget.observe(selection_change_handler, names=[\"selection\"])\n",
        "\n",
        "ipywidgets.HBox([scatter.show(), output])"
      ],
      "metadata": {
        "id": "gR8HOvEG0Dvu"
      },
      "id": "gR8HOvEG0Dvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example de visualisation:\n",
        "\n",
        "https://projector.tensorflow.org/\n",
        "\n",
        "https://helboukkouri.github.io/embedding-visualization/"
      ],
      "metadata": {
        "id": "ul82-aMvmqPt"
      },
      "id": "ul82-aMvmqPt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque dimension correspond à un \"concept\" utilisé par le modèle pour différencier les *tokens* entre eux.\n",
        "\n",
        "Chaque cellule d’un *embedding* correspond alors à la position, allant de -1 à 1, du token associé sur une des dimensions de l’espace latent du modèle.\n",
        "\n",
        "Nous pouvons, par exemple, visualiser la position d’un token sur chaque dimension à l’aide d’une *heatmap*."
      ],
      "metadata": {
        "id": "fgxQJCIQ3nG_"
      },
      "id": "fgxQJCIQ3nG_"
    },
    {
      "cell_type": "code",
      "source": [
        "# On commence par convertir notre embedding dans un format plus adapté\n",
        "df = df_embedded_vocab.iloc[[5271]]\n",
        "\n",
        "# Et on lance la représentation (des 10 premières valeurs pour plus de lisibilité)\n",
        "# sous forme de heatmap\n",
        "sb.heatmap(df.iloc[:, : 10], vmin=-1.0, vmax=1.0, annot=True, fmt=\".2f\")"
      ],
      "metadata": {
        "id": "v--Rw8sI5gQx"
      },
      "id": "v--Rw8sI5gQx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exemples token proches\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "tst = df_embedded_vocab.loc[[sentence]]\n",
        "\n",
        "df_tst = pd.DataFrame(cosine_similarity(tst, df_embedded_vocab)[0], columns=[\"similarity\"], index=vocab)\n",
        "df_tst = df_tst.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "df_tst"
      ],
      "metadata": {
        "id": "HgeHyceH6lSJ"
      },
      "id": "HgeHyceH6lSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voir:\n",
        "\n",
        "https://degaucheoudedroite.delemazure.fr/"
      ],
      "metadata": {
        "id": "FyXMs6NKvZa1"
      },
      "id": "FyXMs6NKvZa1"
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO Exemples math sur vecteur (ROI - HOMME + FEMME)\n",
        "mot1 = pipeline(\"France\")\n",
        "mot1 = pd.DataFrame([np.array(mot1[0]).mean(axis=0)])\n",
        "mot1"
      ],
      "metadata": {
        "id": "x94h2ddZ8Nkb"
      },
      "id": "x94h2ddZ8Nkb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mot2 = pipeline(\"Camembert\")\n",
        "mot2 = pd.DataFrame([np.array(mot2[0]).mean(axis=0)])\n",
        "mot2"
      ],
      "metadata": {
        "id": "S9t9RWFxYpLl"
      },
      "id": "S9t9RWFxYpLl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mot3 = pipeline(\"Mozzarella\")\n",
        "mot3 = pd.DataFrame([np.array(mot3[0]).mean(axis=0)])\n",
        "mot3"
      ],
      "metadata": {
        "id": "6O-JSYTZrIsi"
      },
      "id": "6O-JSYTZrIsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst = mot1 - mot2 + mot3\n",
        "tst"
      ],
      "metadata": {
        "id": "Wi-BMnx7xxnH"
      },
      "id": "Wi-BMnx7xxnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tst = pd.DataFrame(cosine_similarity(tst, df_embedded_vocab)[0], columns=[\"similarity\"], index=vocab)\n",
        "df_tst = df_tst.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "df_tst.iloc[:10]"
      ],
      "metadata": {
        "id": "p6dWjkZlZOsY"
      },
      "id": "p6dWjkZlZOsY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voir: http://nlp.polytechnique.fr/word2vec\n",
        "\n",
        "https://neal.fun/infinite-craft/"
      ],
      "metadata": {
        "id": "S36fM4imfuJC"
      },
      "id": "S36fM4imfuJC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa",
      "metadata": {
        "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa"
      },
      "outputs": [],
      "source": [
        "del embedded_vocab\n",
        "del df_embedded_vocab\n",
        "del vocab\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour en savoir plus sur le fonctionnement des embeddings, voir: https://jalammar.github.io/illustrated-word2vec/"
      ],
      "metadata": {
        "id": "J1doVg9C2UN2"
      },
      "id": "J1doVg9C2UN2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Sauvegarde et recherche sémantique de documents"
      ],
      "metadata": {
        "id": "pwTD9GCa6COG"
      },
      "id": "pwTD9GCa6COG"
    },
    {
      "cell_type": "code",
      "source": [
        "data = pipeline(\"J'aime le fromage\")\n",
        "pd.DataFrame(data[0])"
      ],
      "metadata": {
        "id": "6oSI3J596PML"
      },
      "id": "6oSI3J596PML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pipeline(\"J'aime le fromage\",return_tensors = \"pt\")[0].numpy().mean(axis=0)\n",
        "data1 = pd.DataFrame([data1])\n",
        "data1"
      ],
      "metadata": {
        "id": "1EC0VBVPvyjr"
      },
      "id": "1EC0VBVPvyjr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pipeline(\"Je déteste le fromage\",return_tensors = \"pt\")[0].numpy().mean(axis=0)\n",
        "data2 = pd.DataFrame([data2])\n",
        "data2"
      ],
      "metadata": {
        "id": "1RNsdNSsi42A"
      },
      "id": "1RNsdNSsi42A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = pipeline(\"Le fromage, c'est vraiment bon\", return_tensors = \"pt\")[0].numpy().mean(axis=0)\n",
        "data3 = pd.DataFrame([data3])\n",
        "data3"
      ],
      "metadata": {
        "id": "9QFrIppNv8lK"
      },
      "id": "9QFrIppNv8lK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data4 = pipeline(\"Le soleil brille sur la ville de Paris aujourd'hui\", return_tensors = \"pt\")[0].numpy().mean(axis=0)\n",
        "data4 = pd.DataFrame([data4])\n",
        "data4"
      ],
      "metadata": {
        "id": "d_eNo4rLwadf"
      },
      "id": "d_eNo4rLwadf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(data1, data2)"
      ],
      "metadata": {
        "id": "1OoYI6v60xnW"
      },
      "id": "1OoYI6v60xnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(data1, data3)"
      ],
      "metadata": {
        "id": "axTK-_LH2Qjm"
      },
      "id": "axTK-_LH2Qjm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(data1, data4)"
      ],
      "metadata": {
        "id": "XhLOYrlB2U8B"
      },
      "id": "XhLOYrlB2U8B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_embeddings = pd.DataFrame(columns=data1.columns)\n",
        "bdd_texts = []\n",
        "\n",
        "def store_text_embeddings(text):\n",
        "  if not text in bdd_texts:\n",
        "    embeddings = pipeline(text, return_tensors = \"pt\")[0].numpy().mean(axis=0)\n",
        "    bdd_embeddings.loc[len(bdd_embeddings.index)] = embeddings\n",
        "    bdd_texts.append(text)"
      ],
      "metadata": {
        "id": "8djvNscO4aGG"
      },
      "id": "8djvNscO4aGG",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "\"Camembert est une appellation générique qui désigne généralement un fromage à pâte molle et à croûte fleurie. Commercialement, cette appellation d'origine normande ne fait l'objet d'aucune protection et se voit utilisée pour des fromages n'ayant parfois que peu de rapport avec le camembert originel. Dans certaines régions de France, le camembert est appelé « claquos », « clacos », « calendos ».\",\n",
        "\"Le bleu d’Auvergne est un fromage à pâte persillée fabriqué en France dans le Massif central à partir de lait de vache. Son persillage allant du bleu au bleu noir. Son appellation d'origine bénéficie de protections depuis 1975.\",\n",
        "\"La tomme de Savoie est un fromage produit en France dans la région alpine de Savoie, regroupant les départements de la Savoie et de la Haute-Savoie. Son appellation est protégée par une indication géographique protégée.\",\n",
        "\"Le munster ou munster-géromé (ou encore Minschterkäs en francique lorrain ou Minschterkaas en alsacien) est un fromage à pâte molle fabriqué à partir de lait de vache dans l'Est de la France. Son appellation est protégée nationalement depuis 1969 par une appellation d'origine contrôlée (AOC) et dans l'ensemble des pays de l'Union européenne depuis 1996 par une appellation d'origine protégée (AOP).\",\n",
        "\"Les bries sont une famille de fromages à pâte molle à croûte fleurie, originaire de la région française de Brie.\",\n",
        "\"L’emmentaler ou emmental est un fromage d'origine suisse à pâte dure dont le nom provient de la vallée de l'Emme (en allemand, Emmental), une région à l'est du canton de Berne.\",\n",
        "\"Le gorgonzola est l'appellation d'origine d'un fromage traditionnel à base de lait de vache, à pâte persillée, fabriqué dans les régions de Lombardie et du Piémont.\",\n",
        "\"Le Valençay est une appellation d'origine désignant un fromage de chèvre au lait cru du Berry, et de la région Centre-Val de Loire, en France. Elle reprend le nom de la commune homonyme.\",\n",
        "\"Le saint-marcellin IGP est un fromage français du Dauphiné. Son Indication géographique protégée (IGP) date de la fin 2013, elle s'étend sur 274 communes en Isère, dans la Drôme et en Savoie.\",\n",
        "\"Reblochon ou reblochon de Savoie est une appellation d'origine désignant un fromage français produit principalement en Haute-Savoie et dans quelques communes de Savoie. Cette appellation est originaire du massif des Bornes et des Aravis, principalement la vallée de Thônes, et s'est étendue au val d'Arly et au massif des Bauges.\",\n",
        "\"Comté est l'appellation d'origine d'un fromage français transformé principalement en Franche-Comté et bénéficiant d'une AOC depuis 1958 et d'une AOP depuis 1996. Son aire de production s'étend dans les départements du Jura, du Doubs, et de l'est de l'Ain. Elle englobe également une commune de Haute-Savoie et quelques-unes de Saône-et-Loire.\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "  store_text_embeddings(text)"
      ],
      "metadata": {
        "id": "-8rySxsz6-qw"
      },
      "id": "-8rySxsz6-qw",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_embeddings"
      ],
      "metadata": {
        "id": "caWXTz6p9M2l"
      },
      "id": "caWXTz6p9M2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_texts"
      ],
      "metadata": {
        "id": "84plogwW9QI0"
      },
      "id": "84plogwW9QI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_documents(query, nresults=5):\n",
        "  query_embeddings = pd.DataFrame([pipeline(query, return_tensors = \"pt\")[0].numpy().mean(axis=0)])\n",
        "  df = pd.DataFrame(cosine_similarity(query_embeddings, bdd_embeddings)[0], columns=[\"similarity\"])\n",
        "  df = df.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "  indexes = df.iloc[:nresults].index\n",
        "\n",
        "  results = []\n",
        "  for i in indexes:\n",
        "    results.append({\"doc\": bdd_texts[i], \"score\": df.loc[i][\"similarity\"]})\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "0xfhwuJT9Sj4"
      },
      "id": "0xfhwuJT9Sj4",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "Fromage de Savoie\n",
        "\"\"\"\n",
        "\n",
        "find_documents(query)"
      ],
      "metadata": {
        "id": "zx0XKqkfAFMG"
      },
      "id": "zx0XKqkfAFMG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voir: https://www.trychroma.com/"
      ],
      "metadata": {
        "id": "LxKkp1QB46PK"
      },
      "id": "LxKkp1QB46PK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie 2 - Large modèles de langage et techniques de génération de texte"
      ],
      "metadata": {
        "id": "su1CsJk7r3Hw"
      },
      "id": "su1CsJk7r3Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "dbvljPpZsLxV"
      },
      "id": "dbvljPpZsLxV",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", device_map=device, torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n"
      ],
      "metadata": {
        "id": "HwyurfWKvXud"
      },
      "id": "HwyurfWKvXud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Complétion de texte"
      ],
      "metadata": {
        "id": "Oz_623fMq0ur"
      },
      "id": "Oz_623fMq0ur"
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO"
      ],
      "metadata": {
        "id": "0qzZEl--q-F5"
      },
      "id": "0qzZEl--q-F5"
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "hRROCUdrV6RM"
      },
      "id": "hRROCUdrV6RM",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 1,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False\n",
        "}"
      ],
      "metadata": {
        "id": "ubobOdUTV6OV"
      },
      "id": "ubobOdUTV6OV",
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Le fromage c'est vraiment bon, surtout le\""
      ],
      "metadata": {
        "id": "-Pz6NiNtBkeU"
      },
      "id": "-Pz6NiNtBkeU",
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(prompt, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "g4MQJ1GJwT9t"
      },
      "id": "g4MQJ1GJwT9t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gereration_args[\"num_return_sequences\"] = 3"
      ],
      "metadata": {
        "id": "W94U4mPkz_b0"
      },
      "id": "W94U4mPkz_b0",
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(prompt, **generation_args)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "mwq7Qu4uBwq5"
      },
      "id": "mwq7Qu4uBwq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args[\"max_new_tokens\"] = 500"
      ],
      "metadata": {
        "id": "GD3YwIqJCbsF"
      },
      "id": "GD3YwIqJCbsF",
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(prompt, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "d37i4LsS0gc7"
      },
      "id": "d37i4LsS0gc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Agents Conversationnels (ChatBot)"
      ],
      "metadata": {
        "id": "bWk15ZnMq_UL"
      },
      "id": "bWk15ZnMq_UL"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"J'aimerais manger un fromage de Savoie, que me conseille tu ?\"},\n",
        "]"
      ],
      "metadata": {
        "id": "53QJedhmVyT1"
      },
      "id": "53QJedhmVyT1",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "AChtEIkjV_XO"
      },
      "id": "AChtEIkjV_XO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append({\"role\": \"assistant\", \"content\": output[0]['generated_text']})"
      ],
      "metadata": {
        "id": "WqO8h7FSzdRv"
      },
      "id": "WqO8h7FSzdRv",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append({\"role\": \"user\", \"content\": \"Je cherche un fromage plus insolite\"})"
      ],
      "metadata": {
        "id": "aUcoIQyC0C5y"
      },
      "id": "aUcoIQyC0C5y",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "qK3vkGC40VMi"
      },
      "id": "qK3vkGC40VMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "drmFSfpe0Ncs"
      },
      "id": "drmFSfpe0Ncs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"L'utilisateur est un amateur de fromage de longue date. Il attends de toi de découvrir des fromages peu connu et insolites. Acceuille le avec un message en français.\"}\n",
        "]\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "    # pour utiliser le paramètre \"temperature\", passer le paramètre \"do_sample\" à True\n",
        "    #\"temperature\": float(SEED)\n",
        "}\n",
        "\n",
        "usr_input = \"\"\n",
        "while usr_input != \"fin\":\n",
        "  output = pipe(messages, **generation_args)\n",
        "  print(\"\\nChatBot:\", output[0]['generated_text'], \"\\n\")\n",
        "  usr_input = input(\"Utilisateur:\")\n",
        "\n",
        "  messages.append({\"role\": \"assistant\", \"content\": output[0]['generated_text']})\n",
        "  messages.append({\"role\": \"user\", \"content\": usr_input})\n",
        "\n"
      ],
      "metadata": {
        "id": "k-x36KUr1IuW"
      },
      "id": "k-x36KUr1IuW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Techniques de génération de textes (Prompt Engineering)"
      ],
      "metadata": {
        "id": "b9YCzP_FrQJM"
      },
      "id": "b9YCzP_FrQJM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoT: https://www.promptingguide.ai/techniques/cot"
      ],
      "metadata": {
        "id": "mXXDiTXHra-p"
      },
      "id": "mXXDiTXHra-p"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Détaille ta réponse étape par étape.\n",
        "          Je cherche un fromage de Savoie, que je conseille-tu?\n",
        "        \"\"\"\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "vgIFgepu8ZwT"
      },
      "id": "vgIFgepu8ZwT",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "QvPm8r3E9Stk"
      },
      "id": "QvPm8r3E9Stk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ToT: https://www.promptingguide.ai/techniques/tot"
      ],
      "metadata": {
        "id": "Sh75-Bl-9dM_"
      },
      "id": "Sh75-Bl-9dM_"
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 5000,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "    # pour utiliser le paramètre \"temperature\", passer le paramètre \"do_sample\" à True\n",
        "    #\"temperature\": float(SEED)\n",
        "}\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Imagine que trois experts différents débattent pour répondre à cette question.\n",
        "          Chaque expert écrit une étape de sa réflexion et la partage avec le groupe.\n",
        "          Puis tous les experts continue ainsi de suite, étape par étape.\n",
        "          Si un expert réalise qu'il s'est trompé à un moment de sa réflexion, il sort de la discussion.\n",
        "          La question est: \"Je cherche un fromage de Savoie, que je conseille-tu?\"\n",
        "        \"\"\"\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "csa3lmHv9cum"
      },
      "id": "csa3lmHv9cum",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "7c-1UNDg-rj-"
      },
      "id": "7c-1UNDg-rj-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GKP: https://www.promptingguide.ai/techniques/knowledge"
      ],
      "metadata": {
        "id": "7oOdIABF_xHJ"
      },
      "id": "7oOdIABF_xHJ"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Avant de répondre à la question, écrit les différentes connaissances que tu as sur le sujet.\n",
        "          La question est: \"Je cherche un fromage de Savoie, que je conseille-tu?\"\n",
        "        \"\"\"\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "o4LAwdWA_lf6"
      },
      "id": "o4LAwdWA_lf6",
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "rBus3JEAAEpr"
      },
      "id": "rBus3JEAAEpr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Avant de répondre à la question, commence par écrire les différentes connaissances que tu as sur le sujet.\n",
        "          Ensuite, imagine que trois experts différents débattent pour répondre à cette question.\n",
        "          Chaque expert écrit une étape de sa réflexion et la partage avec le groupe.\n",
        "          Puis tous les experts continue ainsi de suite, étape par étape.\n",
        "          Si un expert réalise qu'il s'est trompé à un moment de sa réflexion, il sort de la discussion.\n",
        "          La question est: \"Je cherche un fromage de Savoie, que je conseille-tu?\"\n",
        "        \"\"\"\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "ER2J7N-XAZAN"
      },
      "id": "ER2J7N-XAZAN",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "IFjwCI3nAmWb"
      },
      "id": "IFjwCI3nAmWb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie 3 - Combiner encodage et décodage avec les LLM Agents"
      ],
      "metadata": {
        "id": "zbtuxkqyrcIS"
      },
      "id": "zbtuxkqyrcIS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO RAG"
      ],
      "metadata": {
        "id": "hLBVRBdirrgW"
      },
      "id": "hLBVRBdirrgW"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}