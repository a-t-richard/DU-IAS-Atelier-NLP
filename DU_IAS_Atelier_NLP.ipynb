{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3",
      "metadata": {
        "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3"
      },
      "source": [
        "# DU IA et Santé - Atelier NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249e0319-7ea9-4c04-855f-8cdc547c134b",
      "metadata": {
        "id": "249e0319-7ea9-4c04-855f-8cdc547c134b"
      },
      "source": [
        "Dans cet atelier, nous allons voir plusieurs techniques de NLP \"moderne\", principalement basées sur les transformers et la librairie [huggingface](https://huggingface.co/).\n",
        "\n",
        "Cet atelier est diviser en trois partie.\n",
        "\n",
        "Dans la partie 1, nous allons nous intéresser à la partie encodage des transformers (BERT et cie.) afin de mieux comprendre les représentations internes de ceux-ci.\n",
        "\n",
        "Dans la partie 2, nous interesserons à la partie decodage des transformers (GPT et cie.) et aux méthodes de *prompt engineering* permettant d’améliorer leurs résultats.\n",
        "\n",
        "Enfin, dans la partie 3, nous allons voir comment méler les deux approches pour construire des assistants personnels.\n",
        "\n",
        "Mais, avant toute choses, il nous faut installer quelques librairies.\n",
        "\n",
        "Pour cela, executez la cellule suivante:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate jupyter-scatter"
      ],
      "metadata": {
        "id": "mVi_HlGyzznr"
      },
      "id": "mVi_HlGyzznr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis, nous allons importer quelques librairies et vérifier que nous accédons bien au GPU (si vous en avez un).\n",
        "\n",
        "Pour cela exécutez la cellule ci-dessous."
      ],
      "metadata": {
        "id": "RLu0vhmeu8Tm"
      },
      "id": "RLu0vhmeu8Tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0",
      "metadata": {
        "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0"
      },
      "outputs": [],
      "source": [
        "# Librairies utilitaires\n",
        "from tqdm.auto import tqdm\n",
        "import ipywidgets\n",
        "import gc\n",
        "\n",
        "# Librairies mathématiques\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jscatter\n",
        "\n",
        "# Librairies ML\n",
        "import torch\n",
        "print(\"cuda available?\", str(torch.cuda.is_available()))\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  torch.cuda.set_device(device)\n",
        "  print(\"cuda version:\", torch.version.cuda)\n",
        "  print(\"cuDNN enabled?\", torch.backends.cudnn.enabled)\n",
        "  print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
        "  print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "# Initialisation générateurs de nombres aléatoires\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.random.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916",
      "metadata": {
        "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916"
      },
      "source": [
        "Si le résultat est : `cuda available? False`, cela signifie que la librairie *torch* n’a pas pu accéder à votre GPU.\n",
        "\n",
        "Dans le cas contraire (ou si vous n’avez pas de GPU à disposition), nous pouvons passer à la suite.\n",
        "\n",
        "remarque: si vous utiliser ce GoogleCollab, cliquez sur Exécution > Modifier le type d’exécution et séléctionnnez l’option T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e",
      "metadata": {
        "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e"
      },
      "source": [
        "## Partie 1 - Encodage et représentations internes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8266d061-7137-4370-b8cd-32a6b6023657",
      "metadata": {
        "id": "8266d061-7137-4370-b8cd-32a6b6023657"
      },
      "source": [
        "Dans cette partie, nous allons voir comment les transformers, plus spécifiquement les modèles de type BERT, encodent et transforment les textes fournis pour les donner en entrée des modèles. Nous allons aussi étudier les représentations internes de ce modèles, leurs espaces latents, et comment manipuler ces réprésentations.\n",
        "\n",
        "Pour cela, nous allons nous baser sur le modèle [CamemBERT](https://camembert-model.fr/), entrainé sur des textes en français.\n",
        "\n",
        "Pour essayer un autre modèle, voir: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55",
      "metadata": {
        "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55"
      },
      "outputs": [],
      "source": [
        "from transformers import CamembertModel, CamembertTokenizer\n",
        "\n",
        "model_name = \"camembert/camembert-base\"\n",
        "\n",
        "model = CamembertModel.from_pretrained(model_name, device_map=device, torch_dtype=\"auto\")\n",
        "tokenizer = CamembertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ded393-8743-4ab4-994d-93b5221c0898",
      "metadata": {
        "id": "91ded393-8743-4ab4-994d-93b5221c0898"
      },
      "source": [
        "### 1.1 Tokenization et encodage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735",
      "metadata": {
        "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735"
      },
      "source": [
        "La première étape pour permettre à un modèle de language de traiter du texte est la tokenisation.\n",
        "\n",
        "Cette étape permet de découper une phrase ou un mot en un ou plusieurs *token* connus du modèle (l’ensemble des tokens connus par un modèle est appelé son \"vocabulaire\").\n",
        "\n",
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986a470a-ee07-49b0-901f-5bdd184df2ea",
      "metadata": {
        "id": "986a470a-ee07-49b0-901f-5bdd184df2ea"
      },
      "outputs": [],
      "source": [
        "sentence = \"fromage\"\n",
        "tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "tokenized_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8238822f-40b5-4402-90ce-80fa206804e7",
      "metadata": {
        "id": "8238822f-40b5-4402-90ce-80fa206804e7"
      },
      "source": [
        "Cependant, un modèle attend en entrée non pas une liste de tokens, mais la liste des id correspondant à ces tokens dans son vocabulaire.\n",
        "\n",
        "Ainsi, il nous faut encoder les tokens obtenus précédemment en une liste d’entier utilisable par le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16",
      "metadata": {
        "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16"
      },
      "outputs": [],
      "source": [
        "encoded_sentence = tokenizer.encode(tokenized_sentence, return_tensors=\"pt\")\n",
        "encoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b",
      "metadata": {
        "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b"
      },
      "source": [
        "Le résultat ci-dessus présente la liste des ids correspondant aux tokens de notre phrase.\n",
        "\n",
        "Nous pouvons cependant noter la précense de deux ids supplémentaires (5 et 6), ceux-ci correspondants aux tokens de début et fin de phrase.\n",
        "\n",
        "Sentez-vous libre de modifier la phrase d’exemple avant de passer à la suite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5844028a-a654-493d-b718-4548f96e6d93",
      "metadata": {
        "id": "5844028a-a654-493d-b718-4548f96e6d93"
      },
      "source": [
        "### 1.2 Word Embeddings et espaces latents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327",
      "metadata": {
        "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327"
      },
      "source": [
        "Maintenant que nous avons comment tokeniser et encoder un texte pour permettre à un modèle de manipuler ce texte, nous allons voir les réprentations internes utilisé par ces modèles pour manipuler ces textes.\n",
        "\n",
        "Pour cela nous allons utiliser la classe *pipeline* de la librairie *transformers*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "29f66090-2173-4de3-aa30-96204336c294",
      "metadata": {
        "id": "29f66090-2173-4de3-aa30-96204336c294"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8678d1-a6ec-4986-8952-81d3047ac812",
      "metadata": {
        "id": "8f8678d1-a6ec-4986-8952-81d3047ac812"
      },
      "source": [
        "Cette classe permet d’utiliser nos modèles sur divers problèmes.\n",
        "\n",
        "Ici, nous allons l’utiliser pour effectuer de la *feature-extraction* et ainsi obtenir les représentations interne des phrases fournies à notre modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "sSe6E8H-zDpe",
      "metadata": {
        "id": "sSe6E8H-zDpe"
      },
      "outputs": [],
      "source": [
        "pipeline = pipeline('feature-extraction', model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b",
      "metadata": {
        "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b"
      },
      "source": [
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BTW_AQGpLPFU",
      "metadata": {
        "id": "BTW_AQGpLPFU"
      },
      "outputs": [],
      "source": [
        "data = pipeline(sentence)\n",
        "pd.DataFrame([data[0][1]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame([np.array(data[0]).mean(axis=0)])"
      ],
      "metadata": {
        "id": "XVCNmpN7zAX4"
      },
      "id": "XVCNmpN7zAX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6142e726-6475-4eaa-abf3-3319754168b0",
      "metadata": {
        "id": "6142e726-6475-4eaa-abf3-3319754168b0"
      },
      "source": [
        "Nous voyons ici, que notre modèle transforme chaque token de notre phrase en vecteurs de taille:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a",
      "metadata": {
        "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a"
      },
      "outputs": [],
      "source": [
        "len(data[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10a34bf-912a-4042-b18a-af2e1993e220",
      "metadata": {
        "id": "f10a34bf-912a-4042-b18a-af2e1993e220"
      },
      "source": [
        "La taille de ces vecteurs, générallement appellés *embeddings*, determine le nombre de dimensions de l’espace latent de notre modèle.\n",
        "\n",
        "Chaque vecteur correspondant alors à un point dans cet espace.\n",
        "\n",
        "Il est alors possible d’avoir un aperçu du vocabulaire d’un modèle dans son espace latent.\n",
        "\n",
        "Pour cela, il nous faut dans un premier temps recupérer les embeddings de chaque token présent dans le vocabulaire d’un modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QflyI3Rw16Jz",
      "metadata": {
        "id": "QflyI3Rw16Jz"
      },
      "outputs": [],
      "source": [
        "# On récupère le vocabulaire du modèle\n",
        "vocab = list(tokenizer.get_vocab().keys())\n",
        "\n",
        "# On encodage le vocabulaire\n",
        "embedded_vocab = pipeline(vocab, batch_size=32)\n",
        "\n",
        "# On met le résultat sous un format plus facile à utiliser\n",
        "embedded_vocab = pd.DataFrame([ev[0][1] for ev in embedded_vocab], index=vocab)\n",
        "\n",
        "# On affiche le résultat\n",
        "embedded_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut alors récupérer l’embedding d’un token specifique, par exemple:"
      ],
      "metadata": {
        "id": "pvca93cDl0jO"
      },
      "id": "pvca93cDl0jO"
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_vocab.iloc[[5270]]"
      ],
      "metadata": {
        "id": "2lgFd9VztyU0"
      },
      "id": "2lgFd9VztyU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "FUgDe0parTUw"
      },
      "id": "FUgDe0parTUw",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca_embedded_vocab = pca.fit_transform(embedded_vocab)"
      ],
      "metadata": {
        "id": "4YmkUiEFvz8I"
      },
      "id": "4YmkUiEFvz8I",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pca = pd.DataFrame(data = pca_embedded_vocab, columns = ['PC1', 'PC2'], index=vocab)"
      ],
      "metadata": {
        "id": "hRLstL5wwNNR"
      },
      "id": "hRLstL5wwNNR",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scatter = jscatter.Scatter(data=df_pca, x=\"PC1\", y=\"PC2\")\n",
        "\n",
        "output = ipywidgets.Output()\n",
        "\n",
        "@output.capture(clear_output=True)\n",
        "def selection_change_handler(change):\n",
        "  display(df_pca.iloc[change.new])\n",
        "\n",
        "scatter.widget.observe(selection_change_handler, names=[\"selection\"])\n",
        "\n",
        "ipywidgets.HBox([scatter.show(), output])"
      ],
      "metadata": {
        "id": "gR8HOvEG0Dvu"
      },
      "id": "gR8HOvEG0Dvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example de visualisation:\n",
        "\n",
        "https://projector.tensorflow.org/\n",
        "\n",
        "https://helboukkouri.github.io/embedding-visualization/"
      ],
      "metadata": {
        "id": "ul82-aMvmqPt"
      },
      "id": "ul82-aMvmqPt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque dimension correspond à un \"concept\" utilisé par le modèle pour différencier les *tokens* entre eux.\n",
        "\n",
        "Chaque cellule d’un *embedding* correspond alors à la position, allant de -1 à 1, du token associé sur une des dimensions de l’espace latent du modèle.\n",
        "\n",
        "Nous pouvons, par exemple, visualiser la position d’un token sur chaque dimension à l’aide d’une *heatmap*."
      ],
      "metadata": {
        "id": "fgxQJCIQ3nG_"
      },
      "id": "fgxQJCIQ3nG_"
    },
    {
      "cell_type": "code",
      "source": [
        "# On commence par convertir notre embedding dans un format plus adapté\n",
        "df = pd.DataFrame([data[0][1]])\n",
        "\n",
        "# Et on lance la représentation (des 10 premières valeurs pour plus de lisibilité)\n",
        "# sous forme de heatmap\n",
        "sb.heatmap(df.iloc[:, : 10], vmin=-1.0, vmax=1.0, annot=True, fmt=\".2f\")"
      ],
      "metadata": {
        "id": "v--Rw8sI5gQx"
      },
      "id": "v--Rw8sI5gQx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exemples token proches\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "tst = pipeline(\"crépuscule\")\n",
        "tst = pd.DataFrame([tst[0][1]])\n",
        "\n",
        "df_tst = pd.DataFrame(cosine_similarity(tst, embedded_vocab)[0], columns=[\"similarity\"], index=vocab)\n",
        "df_tst = df_tst.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "df_tst.iloc[:10]"
      ],
      "metadata": {
        "id": "HgeHyceH6lSJ"
      },
      "id": "HgeHyceH6lSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voir:\n",
        "\n",
        "https://degaucheoudedroite.delemazure.fr/\n",
        "\n",
        "https://neal.fun/infinite-craft/"
      ],
      "metadata": {
        "id": "FyXMs6NKvZa1"
      },
      "id": "FyXMs6NKvZa1"
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO Exemples math sur vecteur (ROI - HOMME + FEMME)\n",
        "roi = pipeline(\"Roi\")\n",
        "roi = pd.DataFrame([roi[0][1]])\n",
        "\n",
        "homme = pipeline(\"Homme\")\n",
        "homme = pd.DataFrame([homme[0][1]])\n",
        "\n",
        "femme = pipeline(\"Femme\")\n",
        "femme = pd.DataFrame([femme[0][1]])\n",
        "\n",
        "result = roi - homme + femme\n",
        "\n",
        "df_tst = pd.DataFrame(cosine_similarity(result, embedded_vocab)[0], columns=[\"similarity\"], index=vocab)\n",
        "df_tst = df_tst.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "df_tst.iloc[:10]"
      ],
      "metadata": {
        "id": "x94h2ddZ8Nkb"
      },
      "id": "x94h2ddZ8Nkb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roi"
      ],
      "metadata": {
        "id": "Wi-BMnx7xxnH"
      },
      "id": "Wi-BMnx7xxnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roi - homme + femme"
      ],
      "metadata": {
        "id": "PR3pWVhtxzWh"
      },
      "id": "PR3pWVhtxzWh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reine = pipeline(\"Reine\")\n",
        "reine = pd.DataFrame([reine[0][1]])\n",
        "reine"
      ],
      "metadata": {
        "id": "fF0psJ4JyRuC"
      },
      "id": "fF0psJ4JyRuC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa",
      "metadata": {
        "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa"
      },
      "outputs": [],
      "source": [
        "del embedded_vocab\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour en savoir plus sur le fonctionnement des embeddings, voir: https://jalammar.github.io/illustrated-word2vec/"
      ],
      "metadata": {
        "id": "J1doVg9C2UN2"
      },
      "id": "J1doVg9C2UN2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Embeddings de documents (TODO)"
      ],
      "metadata": {
        "id": "pwTD9GCa6COG"
      },
      "id": "pwTD9GCa6COG"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6oSI3J596PML"
      },
      "id": "6oSI3J596PML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie 2 - Large modèles de langage et techniques de génération de texte"
      ],
      "metadata": {
        "id": "su1CsJk7r3Hw"
      },
      "id": "su1CsJk7r3Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "dbvljPpZsLxV"
      },
      "id": "dbvljPpZsLxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", device_map=device, torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n"
      ],
      "metadata": {
        "id": "HwyurfWKvXud"
      },
      "id": "HwyurfWKvXud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Qu'est qu'un cathéter et comment l'utiliser ?\"},\n",
        "]"
      ],
      "metadata": {
        "id": "53QJedhmVyT1"
      },
      "id": "53QJedhmVyT1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "hRROCUdrV6RM"
      },
      "id": "hRROCUdrV6RM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "    # pour utiliser le paramètre \"temperature\", passer le paramètre \"do_sample\" à True\n",
        "    #\"temperature\": float(SEED)\n",
        "}"
      ],
      "metadata": {
        "id": "ubobOdUTV6OV"
      },
      "id": "ubobOdUTV6OV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "AChtEIkjV_XO"
      },
      "id": "AChtEIkjV_XO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}