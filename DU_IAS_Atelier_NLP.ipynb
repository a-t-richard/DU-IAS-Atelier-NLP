{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3",
      "metadata": {
        "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3"
      },
      "source": [
        "# DU IA et Santé - Atelier NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249e0319-7ea9-4c04-855f-8cdc547c134b",
      "metadata": {
        "id": "249e0319-7ea9-4c04-855f-8cdc547c134b"
      },
      "source": [
        "Dans cet atelier, nous allons voir plusieurs techniques de NLP \"moderne\", principalement basées sur les transformers et la librairie [huggingface](https://huggingface.co/).\n",
        "\n",
        "Cet atelier est diviser en trois partie.\n",
        "\n",
        "Dans la partie 1, nous allons nous intéresser à la partie encodage des transformers (BERT et cie.) afin de mieux comprendre les représentations internes de ceux-ci.\n",
        "\n",
        "Dans la partie 2, nous interesserons à la partie decodage des transformers (GPT et cie.) et aux méthodes de *prompt engineering* permettant d’améliorer leurs résultats.\n",
        "\n",
        "Enfin, dans la partie 3, nous allons voir comment méler les deux approches pour construire des assistants personnels.\n",
        "\n",
        "Mais, avant toute choses, il nous faut installer quelques librairies.\n",
        "\n",
        "Pour cela, executez la cellule suivante:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "mVi_HlGyzznr"
      },
      "id": "mVi_HlGyzznr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis, nous allons importer quelques librairies et vérifier que nous accédons bien au GPU (si vous en avez un).\n",
        "\n",
        "Pour cela exécutez la cellule ci-dessous."
      ],
      "metadata": {
        "id": "RLu0vhmeu8Tm"
      },
      "id": "RLu0vhmeu8Tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0",
      "metadata": {
        "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0"
      },
      "outputs": [],
      "source": [
        "# Librairies utilitaires\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "\n",
        "# Librairies ML\n",
        "import torch\n",
        "print(\"cuda available?\", str(torch.cuda.is_available()))\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  torch.cuda.set_device(device)\n",
        "  print(\"cuda version:\", torch.version.cuda)\n",
        "  print(\"cuDNN enabled?\", torch.backends.cudnn.enabled)\n",
        "  print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
        "  print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "# Initialisation générateurs de nombres aléatoires\n",
        "SEED = 42\n",
        "torch.random.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916",
      "metadata": {
        "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916"
      },
      "source": [
        "Si le résultat est : `cuda available? False`, cela signifie que la librairie *torch* n’a pas pu accéder à votre GPU.\n",
        "\n",
        "Dans le cas contraire (ou si vous n’avez pas de GPU à disposition), nous pouvons passer à la suite.\n",
        "\n",
        "remarque: si vous utiliser ce GoogleCollab, cliquez sur Exécution > Modifier le type d’exécution et séléctionnnez l’option T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e",
      "metadata": {
        "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e"
      },
      "source": [
        "## Partie 1 - Encodage et représentations internes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8266d061-7137-4370-b8cd-32a6b6023657",
      "metadata": {
        "id": "8266d061-7137-4370-b8cd-32a6b6023657"
      },
      "source": [
        "Dans cette partie, nous allons voir comment les transformers, plus spécifiquement les modèles de type BERT, encodent et transforment les textes fournis pour les donner en entrée des modèles. Nous allons aussi étudier les représentations internes de ce modèles, leurs espaces latents, et comment manipuler ces réprésentations.\n",
        "\n",
        "Pour cela, nous allons nous baser sur le modèle [CamemBERT](https://camembert-model.fr/), entrainé sur des textes en français.\n",
        "\n",
        "Pour essayer un autre modèle, voir: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55",
      "metadata": {
        "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55"
      },
      "outputs": [],
      "source": [
        "from transformers import CamembertModel, CamembertTokenizer\n",
        "\n",
        "model_name = \"camembert/camembert-base\"\n",
        "\n",
        "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
        "model = CamembertModel.from_pretrained(model_name)\n",
        "\n",
        "model.to(device) # charge le modele sur le CPU/GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ded393-8743-4ab4-994d-93b5221c0898",
      "metadata": {
        "id": "91ded393-8743-4ab4-994d-93b5221c0898"
      },
      "source": [
        "### 1.1 Tokenization et encodage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735",
      "metadata": {
        "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735"
      },
      "source": [
        "La première étape pour permettre à un modèle de language de traiter du texte est la tokenisation.\n",
        "\n",
        "Cette étape permet de découper une phrase ou un mot en un ou plusieurs *token* connus du modèle (l’ensemble des tokens connus par un modèle est appelé son \"vocabulaire\").\n",
        "\n",
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986a470a-ee07-49b0-901f-5bdd184df2ea",
      "metadata": {
        "id": "986a470a-ee07-49b0-901f-5bdd184df2ea"
      },
      "outputs": [],
      "source": [
        "sentence = \"fromage\"\n",
        "tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "tokenized_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8238822f-40b5-4402-90ce-80fa206804e7",
      "metadata": {
        "id": "8238822f-40b5-4402-90ce-80fa206804e7"
      },
      "source": [
        "Cependant, un modèle attend en entrée non pas une liste de tokens, mais la liste des id correspondant à ces tokens dans son vocabulaire.\n",
        "\n",
        "Ainsi, il nous faut encoder les tokens obtenus précédemment en une liste d’entier utilisable par le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16",
      "metadata": {
        "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16"
      },
      "outputs": [],
      "source": [
        "encoded_sentence = tokenizer.encode(tokenized_sentence, return_tensors=\"pt\")\n",
        "encoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b",
      "metadata": {
        "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b"
      },
      "source": [
        "Le résultat ci-dessus présente la liste des ids correspondant aux tokens de notre phrase.\n",
        "\n",
        "Nous pouvons cependant noter la précense de deux ids supplémentaires (5 et 6), ceux-ci correspondants aux tokens de début et fin de phrase.\n",
        "\n",
        "Sentez-vous libre de modifier la phrase d’exemple avant de passer à la suite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5844028a-a654-493d-b718-4548f96e6d93",
      "metadata": {
        "id": "5844028a-a654-493d-b718-4548f96e6d93"
      },
      "source": [
        "### 1.2 Word Embeddings et espaces latents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327",
      "metadata": {
        "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327"
      },
      "source": [
        "Maintenant que nous avons comment tokeniser et encoder un texte pour permettre à un modèle de manipuler ce texte, nous allons voir les réprentations internes utilisé par ces modèles pour manipuler ces textes.\n",
        "\n",
        "Pour cela nous allons utiliser la classe *pipeline* de la librairie *transformers*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f66090-2173-4de3-aa30-96204336c294",
      "metadata": {
        "id": "29f66090-2173-4de3-aa30-96204336c294"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8678d1-a6ec-4986-8952-81d3047ac812",
      "metadata": {
        "id": "8f8678d1-a6ec-4986-8952-81d3047ac812"
      },
      "source": [
        "Cette classe permet d’utiliser nos modèles sur divers problèmes.\n",
        "\n",
        "Ici, nous allons l’utiliser pour effectuer de la *feature-extraction* et ainsi obtenir les représentations interne des phrases fournies à notre modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sSe6E8H-zDpe",
      "metadata": {
        "id": "sSe6E8H-zDpe"
      },
      "outputs": [],
      "source": [
        "pipeline = pipeline('feature-extraction', model=model, tokenizer=tokenizer, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b",
      "metadata": {
        "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b"
      },
      "source": [
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BTW_AQGpLPFU",
      "metadata": {
        "id": "BTW_AQGpLPFU"
      },
      "outputs": [],
      "source": [
        "data = pipeline(sentence)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6142e726-6475-4eaa-abf3-3319754168b0",
      "metadata": {
        "id": "6142e726-6475-4eaa-abf3-3319754168b0"
      },
      "source": [
        "Nous voyons ici, que notre modèle transforme chaque token de notre phrase en vecteurs de taille:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a",
      "metadata": {
        "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a"
      },
      "outputs": [],
      "source": [
        "len(data[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10a34bf-912a-4042-b18a-af2e1993e220",
      "metadata": {
        "id": "f10a34bf-912a-4042-b18a-af2e1993e220"
      },
      "source": [
        "La taille de ces vecteurs, générallement appellés *embeddings*, determine le nombre de dimensions de l’espace latent de notre modèle.\n",
        "\n",
        "Chaque vecteur correspondant alors à un point dans cet espace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QflyI3Rw16Jz",
      "metadata": {
        "id": "QflyI3Rw16Jz"
      },
      "outputs": [],
      "source": [
        "embedded_vocab = pipeline(list(tokenizer.get_vocab().keys()), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa",
      "metadata": {
        "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa"
      },
      "outputs": [],
      "source": [
        "del embedded_vocab\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie 2 - Décodage et génération de texte"
      ],
      "metadata": {
        "id": "su1CsJk7r3Hw"
      },
      "id": "su1CsJk7r3Hw"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "dbvljPpZsLxV"
      },
      "id": "dbvljPpZsLxV",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", device_map=device, torch_dtype=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n"
      ],
      "metadata": {
        "id": "HwyurfWKvXud"
      },
      "id": "HwyurfWKvXud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Qu'est qu'un cathéter et comment l'utiliser ?\"},\n",
        "]"
      ],
      "metadata": {
        "id": "53QJedhmVyT1"
      },
      "id": "53QJedhmVyT1",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "hRROCUdrV6RM"
      },
      "id": "hRROCUdrV6RM",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "    # pour utiliser le paramètre \"temperature\", passer le paramètre \"do_sample\" à True\n",
        "    #\"temperature\": float(SEED)\n",
        "}"
      ],
      "metadata": {
        "id": "ubobOdUTV6OV"
      },
      "id": "ubobOdUTV6OV",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "AChtEIkjV_XO"
      },
      "id": "AChtEIkjV_XO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}