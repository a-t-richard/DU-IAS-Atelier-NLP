{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3",
      "metadata": {
        "id": "273b8034-4bc7-4e2a-8bd7-3e6fd846b0e3"
      },
      "source": [
        "# DU IA et Santé - Atelier NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249e0319-7ea9-4c04-855f-8cdc547c134b",
      "metadata": {
        "id": "249e0319-7ea9-4c04-855f-8cdc547c134b"
      },
      "source": [
        "Dans cet atelier, nous allons voir plusieurs techniques de NLP \"moderne\", principalement basées sur les transformers et la librairie [huggingface](https://huggingface.co/).\n",
        "\n",
        "Cet atelier est diviser en trois partie.\n",
        "\n",
        "Dans la partie 1, nous allons nous intéresser à la partie encodage des transformers (BERT et cie.) afin de mieux comprendre les représentations internes de ceux-ci.\n",
        "\n",
        "Dans la partie 2, nous interesserons à la partie decodage des transformers (GPT et cie.) et aux méthodes de *prompt engineering* permettant d’améliorer leurs résultats.\n",
        "\n",
        "Enfin, dans la partie 3, nous allons voir comment méler les deux approches pour construire des assistants personnels.\n",
        "\n",
        "Mais, avant toute choses, il nous faut installer quelques librairies.\n",
        "\n",
        "Pour cela, executez la cellule suivante:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate flash-attention jupyter-scatter"
      ],
      "metadata": {
        "id": "mVi_HlGyzznr"
      },
      "id": "mVi_HlGyzznr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis, nous allons importer quelques librairies et vérifier que nous accédons bien au GPU (si vous en avez un).\n",
        "\n",
        "Pour cela exécutez la cellule ci-dessous."
      ],
      "metadata": {
        "id": "RLu0vhmeu8Tm"
      },
      "id": "RLu0vhmeu8Tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0",
      "metadata": {
        "id": "ce42f004-ded4-4478-a1cd-bf7abe094dd0"
      },
      "outputs": [],
      "source": [
        "# Librairies utilitaires\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Librairies mathématiques\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Librairies ML\n",
        "import torch\n",
        "print(\"cuda available?\", str(torch.cuda.is_available()))\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  torch.cuda.set_device(device)\n",
        "  print(\"cuda version:\", torch.version.cuda)\n",
        "  print(\"cuDNN enabled?\", torch.backends.cudnn.enabled)\n",
        "  print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
        "  print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "# Initialisation générateurs de nombres aléatoires\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.random.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916",
      "metadata": {
        "id": "800ba238-ad44-4fd9-aebc-efae3e5fb916"
      },
      "source": [
        "Si le résultat est : `cuda available? False`, cela signifie que la librairie *torch* n’a pas pu accéder à votre GPU.\n",
        "\n",
        "Dans le cas contraire (ou si vous n’avez pas de GPU à disposition), nous pouvons passer à la suite.\n",
        "\n",
        "remarque: si vous utiliser ce GoogleCollab, cliquez sur Exécution > Modifier le type d’exécution et séléctionnnez l’option T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e",
      "metadata": {
        "id": "5e01f7b9-779a-4d78-b336-18708fd2f35e"
      },
      "source": [
        "## Partie 1 - Encodage et représentations internes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8266d061-7137-4370-b8cd-32a6b6023657",
      "metadata": {
        "id": "8266d061-7137-4370-b8cd-32a6b6023657"
      },
      "source": [
        "Dans cette partie, nous allons voir comment les transformers, plus spécifiquement les modèles de type BERT, encodent et transforment les textes fournis pour les donner en entrée des modèles. Nous allons aussi étudier les représentations internes de ce modèles, leurs espaces latents, et comment manipuler ces réprésentations.\n",
        "\n",
        "Pour cela, nous allons nous baser sur le modèle [CamemBERT](https://camembert-model.fr/), entrainé sur des textes en français.\n",
        "\n",
        "Si vous souhaitez essayer un autre modèle, voir: https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55",
      "metadata": {
        "id": "7489c776-8a0b-4ca6-9dfd-47a66c48dd55"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "bert_model_name = \"Lajavaness/sentence-camembert-base\"\n",
        "\n",
        "bert_model = AutoModel.from_pretrained(bert_model_name, device_map=device, torch_dtype=\"auto\")\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ded393-8743-4ab4-994d-93b5221c0898",
      "metadata": {
        "id": "91ded393-8743-4ab4-994d-93b5221c0898"
      },
      "source": [
        "### 1.1 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735",
      "metadata": {
        "id": "8272e3de-d4e6-42f4-a63b-f6c772e6a735"
      },
      "source": [
        "La première étape pour permettre à un modèle de language de traiter du texte est la tokenisation.\n",
        "\n",
        "Cette étape permet de découper une phrase ou un mot en un ou plusieurs *token* connus du modèle (l’ensemble des tokens connus par un modèle est appelé son \"vocabulaire\").\n",
        "\n",
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986a470a-ee07-49b0-901f-5bdd184df2ea",
      "metadata": {
        "id": "986a470a-ee07-49b0-901f-5bdd184df2ea"
      },
      "outputs": [],
      "source": [
        "sentence = \"fromage\"\n",
        "tokenized_sentence = bert_tokenizer.tokenize(sentence)\n",
        "tokenized_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8238822f-40b5-4402-90ce-80fa206804e7",
      "metadata": {
        "id": "8238822f-40b5-4402-90ce-80fa206804e7"
      },
      "source": [
        "Cependant, un modèle attend en entrée non pas une liste de tokens, mais la liste des id correspondant à ces tokens dans son vocabulaire.\n",
        "\n",
        "Ainsi, il nous faut encoder les tokens obtenus précédemment en une liste d’entier utilisable par le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16",
      "metadata": {
        "id": "1fd03fce-70cc-4aa0-8904-4cf47a685a16"
      },
      "outputs": [],
      "source": [
        "encoded_sentence = bert_tokenizer.encode(tokenized_sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
        "encoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b",
      "metadata": {
        "id": "da2ad3a8-a883-4c81-b2b2-6ba3af76be6b"
      },
      "source": [
        "Le résultat ci-dessus présente la liste des ids correspondant aux tokens de notre phrase.\n",
        "\n",
        "Nous pouvons cependant noter la précense de deux ids supplémentaires (5 et 6), ceux-ci correspondants aux tokens de début et fin de phrase.\n",
        "\n",
        "Sentez-vous libre de modifier la phrase d’exemple avant de passer à la suite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5844028a-a654-493d-b718-4548f96e6d93",
      "metadata": {
        "id": "5844028a-a654-493d-b718-4548f96e6d93"
      },
      "source": [
        "### 1.2 Word embeddings et espaces latents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327",
      "metadata": {
        "id": "e526fa58-3c07-4a18-a148-de9aa8cf1327"
      },
      "source": [
        "Maintenant que nous avons comment tokeniser et encoder un texte pour permettre à un modèle de manipuler ce texte, nous allons voir les réprentations internes utilisé par ces modèles pour manipuler ces textes.\n",
        "\n",
        "Pour cela nous allons utiliser la classe *pipeline* de la librairie *transformers*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f66090-2173-4de3-aa30-96204336c294",
      "metadata": {
        "id": "29f66090-2173-4de3-aa30-96204336c294"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f8678d1-a6ec-4986-8952-81d3047ac812",
      "metadata": {
        "id": "8f8678d1-a6ec-4986-8952-81d3047ac812"
      },
      "source": [
        "Cette classe permet d’utiliser nos modèles sur divers problèmes.\n",
        "\n",
        "Ici, nous allons l’utiliser pour effectuer de la *feature-extraction* et ainsi obtenir les représentations interne des phrases fournies à notre modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sSe6E8H-zDpe",
      "metadata": {
        "id": "sSe6E8H-zDpe"
      },
      "outputs": [],
      "source": [
        "feature_extractor = pipeline('feature-extraction', model=bert_model, tokenizer=bert_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b",
      "metadata": {
        "id": "bebe3bee-ba79-4b66-81cc-c900a6b7a95b"
      },
      "source": [
        "Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BTW_AQGpLPFU",
      "metadata": {
        "id": "BTW_AQGpLPFU"
      },
      "outputs": [],
      "source": [
        "example_embeddings = feature_extractor(sentence)\n",
        "pd.DataFrame(example_embeddings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6142e726-6475-4eaa-abf3-3319754168b0",
      "metadata": {
        "id": "6142e726-6475-4eaa-abf3-3319754168b0"
      },
      "source": [
        "Nous voyons ici, que notre modèle transforme chaque token de notre mot en vecteurs, ou *embeddings*, de taille:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a",
      "metadata": {
        "id": "4347cbc2-8d72-49a5-95ed-705eff380a3a"
      },
      "outputs": [],
      "source": [
        "len(example_embeddings[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour une meilleure manipulation, il est possible de faire la moyenne de ces vecteurs en un vecteur unique."
      ],
      "metadata": {
        "id": "KqaX3Iywf-Cn"
      },
      "id": "KqaX3Iywf-Cn"
    },
    {
      "cell_type": "code",
      "source": [
        "example_embedding = pd.DataFrame([np.array(example_embeddings[0]).mean(axis=0)])\n",
        "example_embedding"
      ],
      "metadata": {
        "id": "XVCNmpN7zAX4"
      },
      "id": "XVCNmpN7zAX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f10a34bf-912a-4042-b18a-af2e1993e220",
      "metadata": {
        "id": "f10a34bf-912a-4042-b18a-af2e1993e220"
      },
      "source": [
        "La taille de ces vecteurs determine le nombre de dimensions de l’espace latent de notre modèle.\n",
        "\n",
        "Chaque dimension correspond à un \"concept\" utilisé par le modèle pour différencier les *tokens* entre eux.\n",
        "\n",
        "Chaque vecteur correspond alors à un point dans cet espace, et chaque cellule d’un vecteur correspond à la position, allant de -1 à 1, du *token* associé sur\n",
        "une des dimensions de l’espace latent du modèle.\n",
        "\n",
        "Nous pouvons, entre autres, visualiser la position d’un *token* sur chaque dimension à l’aide d’une *heatmap*.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Représentation des 10 premières valeurs pour plus de lisibilité\n",
        "sb.heatmap(example_embedding.iloc[:,:10], vmin=-1.0, vmax=1.0, annot=True, fmt=\".2f\")"
      ],
      "metadata": {
        "id": "v--Rw8sI5gQx"
      },
      "id": "v--Rw8sI5gQx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Il est aussi possible d’avoir un aperçu du vocabulaire d’un modèle dans son espace latent.\n",
        "\n",
        "Pour cela, il nous faut dans un premier temps recupérer les embeddings de chaque token présent dans le vocabulaire d’un modèle."
      ],
      "metadata": {
        "id": "P-6j3QoIg_Hm"
      },
      "id": "P-6j3QoIg_Hm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QflyI3Rw16Jz",
      "metadata": {
        "id": "QflyI3Rw16Jz"
      },
      "outputs": [],
      "source": [
        "# On récupère le vocabulaire du modèle\n",
        "vocab = [token.replace(\"▁\", \"\") for token in list(bert_tokenizer.get_vocab().keys())]\n",
        "\n",
        "# On encodage le vocabulaire\n",
        "embedded_vocab = feature_extractor(vocab, batch_size=32)\n",
        "\n",
        "# On met le résultat sous un format plus facile à utiliser\n",
        "df_embedded_vocab = pd.DataFrame([np.array(ev[0]).mean(axis=0) for ev in embedded_vocab], index=vocab)\n",
        "\n",
        "# On affiche le résultat\n",
        "df_embedded_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut alors récupérer l’embedding d’un token specifique, par exemple:"
      ],
      "metadata": {
        "id": "pvca93cDl0jO"
      },
      "id": "pvca93cDl0jO"
    },
    {
      "cell_type": "code",
      "source": [
        "df_embedded_vocab.loc[[sentence]]"
      ],
      "metadata": {
        "id": "2lgFd9VztyU0"
      },
      "id": "2lgFd9VztyU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il nous faut maintenant représenter ces tokens sur un plan en deux dimensions.\n",
        "\n",
        "En d’autres termes, il nous faut réduire le nombre de dimensions de nos vecteurs\n",
        "(ou *embeddings*) à 2.\n",
        "\n",
        "Pour cela, nous allons utiliser l’algorithme *Principal Component Analysis* (PCA) disponible dans la librairie *scikit-learn*."
      ],
      "metadata": {
        "id": "tcM5qd9Zi6B1"
      },
      "id": "tcM5qd9Zi6B1"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "FUgDe0parTUw"
      },
      "id": "FUgDe0parTUw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans un premier temps, nous allons initialiser PCA et l’appliquer aux vecteurs correspondants au vocabulaire de notre modèle pour déterminer les deux dimensions à utiliser."
      ],
      "metadata": {
        "id": "__WODxdbkiCc"
      },
      "id": "__WODxdbkiCc"
    },
    {
      "cell_type": "code",
      "source": [
        "# On initialise PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# On applique PCA à nos embeddings\n",
        "pca_embedded_vocab = pca.fit_transform(df_embedded_vocab)\n",
        "\n",
        "# Et on mets le résultats sous un format plus facilement manipulable\n",
        "df_pca = pd.DataFrame(data = pca_embedded_vocab, columns = ['PC1', 'PC2'], index=vocab)"
      ],
      "metadata": {
        "id": "4YmkUiEFvz8I"
      },
      "id": "4YmkUiEFvz8I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons maintenant générer un graphe de points en deux dimensions dans lequel chaque point correspond à un token.\n",
        "\n",
        "Pour cela nous utilisons les librairies *jscatter* et *ipywidgets* qui permettent de générer des graphes interactifs."
      ],
      "metadata": {
        "id": "SQmh3TGplZDn"
      },
      "id": "SQmh3TGplZDn"
    },
    {
      "cell_type": "code",
      "source": [
        "import jscatter\n",
        "import ipywidgets"
      ],
      "metadata": {
        "id": "p2ZoEiFem5iT"
      },
      "id": "p2ZoEiFem5iT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du graphe\n",
        "scatter = jscatter.Scatter(data=df_pca, x=\"PC1\", y=\"PC2\")\n",
        "\n",
        "# Création du module interactif\n",
        "output = ipywidgets.Output()\n",
        "\n",
        "# Définition d'une fonction à appeler lors de la sélection de points\n",
        "@output.capture(clear_output=True)\n",
        "def selection_change_handler(change):\n",
        "  display(df_pca.iloc[change.new])\n",
        "\n",
        "# Connection de l'action de sélection de points à la fonction\n",
        "scatter.widget.observe(selection_change_handler, names=[\"selection\"])\n",
        "\n",
        "# Affichage du graphe\n",
        "ipywidgets.HBox([scatter.show(), output])"
      ],
      "metadata": {
        "id": "gR8HOvEG0Dvu"
      },
      "id": "gR8HOvEG0Dvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour d’autres exemples de visualisation d’espace latent, voir:\n",
        "\n",
        "https://projector.tensorflow.org/\n",
        "\n",
        "https://helboukkouri.github.io/embedding-visualization/"
      ],
      "metadata": {
        "id": "ul82-aMvmqPt"
      },
      "id": "ul82-aMvmqPt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque point d’un espace latent étant associé à un token et positionné les uns par rapport aux autres, il est donc théoriquement possible de calculer une \"distance sémantique\" entre deux tokens.\n",
        "\n",
        "Pour cela, nous allons utiliser la [similarité cosinus](https://fr.wikipedia.org/wiki/Similarit%C3%A9_cosinus) disponible dans la librairie *scikit-learn*."
      ],
      "metadata": {
        "id": "nLoFhrI3mdg8"
      },
      "id": "nLoFhrI3mdg8"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "yMaa-W6G_AFb"
      },
      "id": "yMaa-W6G_AFb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La similarité cosinus se base sur l’angle formé par deux vecteurs de taille $n$ et fourni un score entre -1 et 1.\n",
        "\n",
        "C’est une similarité généralement employée en TAL.\n",
        "\n",
        "Nous allons donc l’employer ici pour trier le vocabulaire de notre modèle et trouver les tokens \"proches\" d’un embedding donné."
      ],
      "metadata": {
        "id": "DZUJYkgToYZ-"
      },
      "id": "DZUJYkgToYZ-"
    },
    {
      "cell_type": "code",
      "source": [
        "### Définition de la fonction de tri\n",
        "def sort_vocab_by_similarity(embedding):\n",
        "  # On calcule la similarite entre l'embedding fourni en entree\n",
        "  # et les embeddings du vocabulaire de notre modèle\n",
        "  cos_sim = cosine_similarity(embedding, df_embedded_vocab)\n",
        "\n",
        "  # On met les résultats sous un format plus facilement manipulable\n",
        "  df = pd.DataFrame(cos_sim[0], columns=[\"similarity\"], index=vocab)\n",
        "\n",
        "  # On tri par similarite avec l'embedding fourni en entree\n",
        "  df = df.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "  # On retourne le résultat\n",
        "  return df"
      ],
      "metadata": {
        "id": "HgeHyceH6lSJ"
      },
      "id": "HgeHyceH6lSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Par exemple:"
      ],
      "metadata": {
        "id": "MAxpZ24vrAX2"
      },
      "id": "MAxpZ24vrAX2"
    },
    {
      "cell_type": "code",
      "source": [
        "sort_vocab_by_similarity(df_embedded_vocab.loc[[sentence]])"
      ],
      "metadata": {
        "id": "F3B6tigpqzHp"
      },
      "id": "F3B6tigpqzHp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C’est cette méthode qui est employé sur des sites tels que:\n",
        "\n",
        "https://cemantix.certitudes.org/\n",
        "\n",
        "https://degaucheoudedroite.delemazure.fr/"
      ],
      "metadata": {
        "id": "FyXMs6NKvZa1"
      },
      "id": "FyXMs6NKvZa1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfin, il est aussi possible d’effectuer des opérations mathématiques sur nos vecteurs.\n",
        "\n",
        "Nous avons déjà vu que nous pouvons faire une moyenne de plusieurs *embeddings*, voyons maintenant ce qu’il se passe lorsque nous additionnons et soustrayons des *embeddings* entre eux."
      ],
      "metadata": {
        "id": "PqBAVIPdrnvp"
      },
      "id": "PqBAVIPdrnvp"
    },
    {
      "cell_type": "code",
      "source": [
        "mot1 = feature_extractor(\"France\")\n",
        "mot1 = pd.DataFrame([np.array(mot1[0]).mean(axis=0)])\n",
        "mot1"
      ],
      "metadata": {
        "id": "x94h2ddZ8Nkb"
      },
      "id": "x94h2ddZ8Nkb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mot2 = feature_extractor(\"Camembert\")\n",
        "mot2 = pd.DataFrame([np.array(mot2[0]).mean(axis=0)])\n",
        "mot2"
      ],
      "metadata": {
        "id": "S9t9RWFxYpLl"
      },
      "id": "S9t9RWFxYpLl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mot3 = feature_extractor(\"Mozzarella\")\n",
        "mot3 = pd.DataFrame([np.array(mot3[0]).mean(axis=0)])\n",
        "mot3"
      ],
      "metadata": {
        "id": "6O-JSYTZrIsi"
      },
      "id": "6O-JSYTZrIsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mot1 - mot2 + mot3"
      ],
      "metadata": {
        "id": "SVin0ip3SpQ5"
      },
      "id": "SVin0ip3SpQ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sort_vocab_by_similarity(mot1 - mot2 + mot3)[:10]"
      ],
      "metadata": {
        "id": "Wi-BMnx7xxnH"
      },
      "id": "Wi-BMnx7xxnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C’est ce principe d’addition de \"concept\" qui est utilisé, par exemple, dans: https://neal.fun/infinite-craft/\n",
        "\n",
        "Pour d’autres exemples employant d’autres modèles, voir: http://nlp.polytechnique.fr/word2vec"
      ],
      "metadata": {
        "id": "S36fM4imfuJC"
      },
      "id": "S36fM4imfuJC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant de passer à la suite, nous allons libérer des éléments de la mémoire de notre session."
      ],
      "metadata": {
        "id": "XheKahuOs-2G"
      },
      "id": "XheKahuOs-2G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa",
      "metadata": {
        "id": "410ffeb0-f76d-49ec-b277-d7b6b51a8daa"
      },
      "outputs": [],
      "source": [
        "del embedded_vocab\n",
        "del df_embedded_vocab\n",
        "del vocab\n",
        "del scatter\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et pour en savoir plus sur le fonctionnement des embeddings, voir: https://jalammar.github.io/illustrated-word2vec/"
      ],
      "metadata": {
        "id": "J1doVg9C2UN2"
      },
      "id": "J1doVg9C2UN2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Sauvegarde et recherche sémantique de documents"
      ],
      "metadata": {
        "id": "pwTD9GCa6COG"
      },
      "id": "pwTD9GCa6COG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la section précédente, nous avons vu comment transformer des mots sous forme de vecteurs et comment manipuler ces vecteurs.\n",
        "\n",
        "Nous allons maintenant voir comment utiliser ces méthodes pour manipuler des phrases et des textes plus complexes.\n",
        "\n",
        "De la même manière qu’un mot, une phrase va être découpé en *tokens* et chaque *token* transformé en *embedding*."
      ],
      "metadata": {
        "id": "XQyJnNsLtYzh"
      },
      "id": "XQyJnNsLtYzh"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(feature_extractor(\"J'aime le fromage\")[0])"
      ],
      "metadata": {
        "id": "6oSI3J596PML"
      },
      "id": "6oSI3J596PML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien qu’il existe plusieurs approches possibles, la méthode généralement pour transformer une phrase, un texte ou un document en un vecteur est simplement de faire la moyenne des *embeddings*.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "YxOn_kDwuPV_"
      },
      "id": "YxOn_kDwuPV_"
    },
    {
      "cell_type": "code",
      "source": [
        "example1 = pd.DataFrame([feature_extractor(\"J'aime le fromage\",return_tensors = \"pt\")[0].numpy().mean(axis=0)])\n",
        "example1"
      ],
      "metadata": {
        "id": "1EC0VBVPvyjr"
      },
      "id": "1EC0VBVPvyjr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example2 = pd.DataFrame([feature_extractor(\"Je déteste le fromage\",return_tensors = \"pt\")[0].numpy().mean(axis=0)])\n",
        "example2"
      ],
      "metadata": {
        "id": "1RNsdNSsi42A"
      },
      "id": "1RNsdNSsi42A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example3 = pd.DataFrame([feature_extractor(\"Le fromage, c'est vraiment bon\", return_tensors = \"pt\")[0].numpy().mean(axis=0)])\n",
        "example3"
      ],
      "metadata": {
        "id": "9QFrIppNv8lK"
      },
      "id": "9QFrIppNv8lK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example4 = pd.DataFrame([feature_extractor(\"Le soleil brille sur la ville de Lyon aujourd'hui\", return_tensors = \"pt\")[0].numpy().mean(axis=0)])\n",
        "example4"
      ],
      "metadata": {
        "id": "d_eNo4rLwadf"
      },
      "id": "d_eNo4rLwadf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et de la même manière que pour les mots, il est possible de calculer une \"distance sémantique\" entre différents textes."
      ],
      "metadata": {
        "id": "SODfD9w5vPez"
      },
      "id": "SODfD9w5vPez"
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(example1, example2)"
      ],
      "metadata": {
        "id": "1OoYI6v60xnW"
      },
      "id": "1OoYI6v60xnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(example1, example3)"
      ],
      "metadata": {
        "id": "axTK-_LH2Qjm"
      },
      "id": "axTK-_LH2Qjm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(example1, example4)"
      ],
      "metadata": {
        "id": "XhLOYrlB2U8B"
      },
      "id": "XhLOYrlB2U8B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En se basant sur ce principe, il est alors possible de stocker des textes et leurs *embeddings* afin de les retrouver en fonction de leur \"similarité\" avec une requête données.\n",
        "\n",
        "Pour cela, il nous faut d’abord créer une fonction permettant de stocker des textes et leurs *embeddings*."
      ],
      "metadata": {
        "id": "mAbnmsXGvooo"
      },
      "id": "mAbnmsXGvooo"
    },
    {
      "cell_type": "code",
      "source": [
        "# On cree une liste d'embeddings vide\n",
        "bdd_embeddings = pd.DataFrame(columns=example1.columns)\n",
        "\n",
        "# On fait de meme pour stocker les textes\n",
        "bdd_texts = []\n",
        "\n",
        "def store_text_embeddings(text):\n",
        "  # Au cas ou, on test si le texte n'existe pas deja\n",
        "  if not text in bdd_texts:\n",
        "    # On genere les embeddings\n",
        "    embeddings = feature_extractor(text, return_tensors = \"pt\")[0].numpy().mean(axis=0)\n",
        "    # On ajoute les embeddings a notre liste d'embeddings\n",
        "    bdd_embeddings.loc[len(bdd_embeddings.index)] = embeddings\n",
        "    # On ajoute notre texte a notre liste de textes\n",
        "    bdd_texts.append(text)"
      ],
      "metadata": {
        "id": "8djvNscO4aGG"
      },
      "id": "8djvNscO4aGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant utiliser cette fonction pour stocker en mémoire différents textes.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "elXe_KWw8CaI"
      },
      "id": "elXe_KWw8CaI"
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "\"Camembert est une appellation générique qui désigne généralement un fromage à pâte molle et à croûte fleurie. Commercialement, cette appellation d'origine normande ne fait l'objet d'aucune protection et se voit utilisée pour des fromages n'ayant parfois que peu de rapport avec le camembert originel. Dans certaines régions de France, le camembert est appelé « claquos », « clacos », « calendos ».\",\n",
        "\"Le bleu d’Auvergne est un fromage à pâte persillée fabriqué en France dans le Massif central à partir de lait de vache. Son persillage allant du bleu au bleu noir. Son appellation d'origine bénéficie de protections depuis 1975.\",\n",
        "\"La tomme de Savoie est un fromage produit en France dans la région alpine de Savoie, regroupant les départements de la Savoie et de la Haute-Savoie. Son appellation est protégée par une indication géographique protégée.\",\n",
        "\"Le munster ou munster-géromé (ou encore Minschterkäs en francique lorrain ou Minschterkaas en alsacien) est un fromage à pâte molle fabriqué à partir de lait de vache dans l'Est de la France. Son appellation est protégée nationalement depuis 1969 par une appellation d'origine contrôlée (AOC) et dans l'ensemble des pays de l'Union européenne depuis 1996 par une appellation d'origine protégée (AOP).\",\n",
        "\"Les bries sont une famille de fromages à pâte molle à croûte fleurie, originaire de la région française de Brie.\",\n",
        "\"L’emmentaler ou emmental est un fromage d'origine suisse à pâte dure dont le nom provient de la vallée de l'Emme (en allemand, Emmental), une région à l'est du canton de Berne.\",\n",
        "\"Beaufort (/bofɔːʁ/) est l'appellation d'origine d'un fromage au lait cru de vache, à pâte pressée cuite, élaboré en Savoie en France. La production du lait et sa transformation s'effectuent dans une aire comprenant la région du Beaufortain d'où il tire son nom. Il est formé en meule à talon légèrement concave. L'appellation beaufort est préservée commercialement via une Appellation d'origine protégée.\",\n",
        "\"Le gorgonzola est l'appellation d'origine d'un fromage traditionnel à base de lait de vache, à pâte persillée, fabriqué dans les régions de Lombardie et du Piémont.\",\n",
        "\"Le Valençay est une appellation d'origine désignant un fromage de chèvre au lait cru du Berry, et de la région Centre-Val de Loire, en France. Elle reprend le nom de la commune homonyme.\",\n",
        "\"Le saint-marcellin IGP est un fromage français du Dauphiné. Son Indication géographique protégée (IGP) date de la fin 2013, elle s'étend sur 274 communes en Isère, dans la Drôme et en Savoie.\",\n",
        "\"Reblochon ou reblochon de Savoie est une appellation d'origine désignant un fromage français produit principalement en Haute-Savoie et dans quelques communes de Savoie. Cette appellation est originaire du massif des Bornes et des Aravis, principalement la vallée de Thônes, et s'est étendue au val d'Arly et au massif des Bauges.\",\n",
        "\"Comté est l'appellation d'origine d'un fromage français transformé principalement en Franche-Comté et bénéficiant d'une AOC depuis 1958 et d'une AOP depuis 1996. Son aire de production s'étend dans les départements du Jura, du Doubs, et de l'est de l'Ain. Elle englobe également une commune de Haute-Savoie et quelques-unes de Saône-et-Loire.\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "  store_text_embeddings(text)"
      ],
      "metadata": {
        "id": "-8rySxsz6-qw"
      },
      "id": "-8rySxsz6-qw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si on affiche nos listes de textes et d’*embeddings*, on voit que les textes et leurs *embeddings* de la cellule précédente ont bien été enregistrés."
      ],
      "metadata": {
        "id": "xp367DKw8Yol"
      },
      "id": "xp367DKw8Yol"
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_texts"
      ],
      "metadata": {
        "id": "84plogwW9QI0"
      },
      "id": "84plogwW9QI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_embeddings"
      ],
      "metadata": {
        "id": "caWXTz6p9M2l"
      },
      "id": "caWXTz6p9M2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, il nous faut créer une fonction permettant de rechercher dans ces *embeddings* pour trouver les documents stockés les plus \"similaires\" à une requête."
      ],
      "metadata": {
        "id": "bp7ld0dE8u-d"
      },
      "id": "bp7ld0dE8u-d"
    },
    {
      "cell_type": "code",
      "source": [
        "def find_embedded_texts(query, nresults=5):\n",
        "  # On genere l'embedding correspondant a la requete\n",
        "  query_embeddings = pd.DataFrame([feature_extractor(query, return_tensors = \"pt\")[0].numpy().mean(axis=0)])\n",
        "\n",
        "  # On calcule la distance entre l'embedding de la requete et les embeddings stockes dans notre liste\n",
        "  df = pd.DataFrame(cosine_similarity(query_embeddings, bdd_embeddings)[0], columns=[\"similarity\"])\n",
        "\n",
        "  # On tri par similarite\n",
        "  df = df.sort_values(by=['similarity'], ascending=False)\n",
        "\n",
        "  # On recupere les indices des premiers resultats\n",
        "  indexes = df.iloc[:nresults].index\n",
        "\n",
        "  # On recupere les textes correspondants\n",
        "  results = []\n",
        "  for i in indexes:\n",
        "    results.append({\"doc\": bdd_texts[i], \"score\": df.loc[i][\"similarity\"]})\n",
        "\n",
        "  # On retourne les textes et leurs scores de similarite\n",
        "  return results"
      ],
      "metadata": {
        "id": "0xfhwuJT9Sj4"
      },
      "id": "0xfhwuJT9Sj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "À l’aide de cette fonction, nous pouvons maintenant rechercher parmis les textes stockées ceux qui correspondent à une requête.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "1wQ8gjlp-R3U"
      },
      "id": "1wQ8gjlp-R3U"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "Fromage de Savoie\n",
        "\"\"\"\n",
        "\n",
        "find_embedded_texts(query)"
      ],
      "metadata": {
        "id": "zx0XKqkfAFMG"
      },
      "id": "zx0XKqkfAFMG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, pour un objectif pédagogique, nous avons codé nous même le fonctionnement d’une base de données utilisant les principe des *embeddings*.\n",
        "\n",
        "Il existe cependant des librairies python, telle que [chroma](https://www.trychroma.com/), qui permettent de gérer plus efficacement de telles bases de données."
      ],
      "metadata": {
        "id": "LxKkp1QB46PK"
      },
      "id": "LxKkp1QB46PK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie 2 - Large modèles de langage et génération de texte"
      ],
      "metadata": {
        "id": "su1CsJk7r3Hw"
      },
      "id": "su1CsJk7r3Hw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette partie, nous allons aborder l’aspect \"decodage\" des transformers.\n",
        "\n",
        "Cet aspect est généralement employé pour générer du texte et est la base des large modèles de langage (LLM) tels que GPT-4, Mistral, ou encore LLaMA.\n",
        "\n",
        "Pour cet atelier, nous allons utiliser le modèle [Phi-3](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/). Un modèle proposé par Microsoft, qui a l’avantage d’être assez léger et sous licence libre (MIT).\n",
        "\n",
        "Afin de charger ce modèles, nous allons utiliser la classe *AutoModelForCausalLM* de la librairie *transformers* comme suit."
      ],
      "metadata": {
        "id": "5VbHOcZNKKSP"
      },
      "id": "5VbHOcZNKKSP"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "dbvljPpZsLxV"
      },
      "id": "dbvljPpZsLxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name, device_map=device, torch_dtype=\"auto\", trust_remote_code=True)\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n"
      ],
      "metadata": {
        "id": "HwyurfWKvXud"
      },
      "id": "HwyurfWKvXud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si vous souhaitez utiliser un autre modèle, voir: https://huggingface.co/models?pipeline_tag=text-generation."
      ],
      "metadata": {
        "id": "Y0jMxcjmLyxu"
      },
      "id": "Y0jMxcjmLyxu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Complétion de texte"
      ],
      "metadata": {
        "id": "Oz_623fMq0ur"
      },
      "id": "Oz_623fMq0ur"
    },
    {
      "cell_type": "markdown",
      "source": [
        "L’objectif de base d’un modèle de langage est la complétion de texte.\n",
        "\n",
        "En d’autres termes, pour un texte données, un llm va chercher à déterminer le *token* pouvant le plus plausiblement faire suite à ce texte.\n",
        "\n",
        "Pour ce faire, nous allons créer un nouveau pipeline permettant de gérérer du texte à partir de notre llm."
      ],
      "metadata": {
        "id": "0qzZEl--q-F5"
      },
      "id": "0qzZEl--q-F5"
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_model,\n",
        "    tokenizer=llm_tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "hRROCUdrV6RM"
      },
      "id": "hRROCUdrV6RM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous alors lui demander de compléter un texte.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "4BC-wH1sOMJy"
      },
      "id": "4BC-wH1sOMJy"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Le fromage c'est vraiment bon, surtout le\""
      ],
      "metadata": {
        "id": "-Pz6NiNtBkeU"
      },
      "id": "-Pz6NiNtBkeU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour ce faire, notre llm va décomposer la phrase en *token* puis en *embeddings* comme vu en Partie 1.\n",
        "\n",
        "Puis il va déterminer, parmis les *tokens* de son vocabulaire, lesquels sont les plus \"plausible\" pour faire suite aux tokens composant le texte fourni en entrée."
      ],
      "metadata": {
        "id": "8R-JDsnUOVen"
      },
      "id": "8R-JDsnUOVen"
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(prompt, max_new_tokens = 1, return_full_text = False, do_sample = True, num_return_sequences=5)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "g4MQJ1GJwT9t"
      },
      "id": "g4MQJ1GJwT9t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parmis ces *tokens*, notre llm va prend le plus \"plausible\", puis l’ajouter aux texte fournit en entrée pour déterminer le *token* suivant.\n",
        "\n",
        "Ainsi de suite, jusqu’à atteindre la longueur souhaitée."
      ],
      "metadata": {
        "id": "7vkgvsudQM7T"
      },
      "id": "7vkgvsudQM7T"
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(prompt, max_new_tokens = 5, return_full_text = False)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "mwq7Qu4uBwq5"
      },
      "id": "mwq7Qu4uBwq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est aussi possible de pousser le llm à sélectioner des *tokens* qui seraient moins \"plausible\", de manière plus aléatoire.\n",
        "\n",
        "Pour cela, il suffit de lui fournir une \"temperature\". Plus cette \"temperature\" est élevée, plus le llm cherchera des *tokens* moins plausible."
      ],
      "metadata": {
        "id": "3z_YburURDqH"
      },
      "id": "3z_YburURDqH"
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(prompt, max_new_tokens= 5, return_full_text = False, do_sample=True, temperature = float(SEED) / 50.0, num_return_sequences=3)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "GD3YwIqJCbsF"
      },
      "id": "GD3YwIqJCbsF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Agents Conversationnels (ChatBot)"
      ],
      "metadata": {
        "id": "bWk15ZnMq_UL"
      },
      "id": "bWk15ZnMq_UL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La complétion de texte que nous avons vu précédemment est la base des agents conversationnels, ou ChatBot, tels que [ChatGPT](https://chatgpt.com/), [Copilot](https://www.bing.com/chat?q=Microsoft+Copilot&FORM=hpcodx) ou encore [ChatCGT](https://chatcgt.fr/).\n",
        "\n",
        "Néanmoins, afin d’optimiser cette complétion dans le cadre d’un ChatBot, les modèles employés sont entrainé sur des textes tournés sous forme de conversations ou \"instructions\".\n",
        "\n",
        "C’est le cas du modèle que nous utilisons. D’ailleurs, si vous augmentez la taille des textes générés dans l’example précédent, notre llm devrait générer une conversation.\n",
        "\n",
        "Afin d’utiliser notre llm dans un ChatBot, il nous donc saisir notre requête en précisant que c’est le texte saisie par l’utilisateur.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "1ApDjtZVm5Au"
      },
      "id": "1ApDjtZVm5Au"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"J'aimerais manger un fromage de Savoie, que me conseille tu ?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": query},\n",
        "]"
      ],
      "metadata": {
        "id": "53QJedhmVyT1"
      },
      "id": "53QJedhmVyT1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(messages, return_full_text = False, max_new_tokens = 500)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "JMyeqEaIo-aM"
      },
      "id": "JMyeqEaIo-aM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cependant, notre llm ne garde pas en mémoire le contexte de la conversation et les questions déjà posées.\n",
        "\n",
        "Pour poursuivre cette conversation, il nous faut donc ajouter les réponses du ChatBot aux messages pré-existant avant de poser une nouvelle question.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "Kh4NN9ewpkS1"
      },
      "id": "Kh4NN9ewpkS1"
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append({\"role\": \"assistant\", \"content\": output[0]['generated_text']})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Je cherche un fromage plus insolite\"})\n",
        "messages"
      ],
      "metadata": {
        "id": "WqO8h7FSzdRv"
      },
      "id": "WqO8h7FSzdRv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(messages, return_full_text = False, max_new_tokens = 500)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "drmFSfpe0Ncs"
      },
      "id": "drmFSfpe0Ncs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "À partir de là, nous avons les éléments pour créer un ChatBot minimal.\n",
        "\n",
        "Tout d’abord, nous allons créer un message qui va initialiser le ChatBot avec ce qui est attendu de lui. Ce message initial est aussi appelé *pre-prompt*.\n"
      ],
      "metadata": {
        "id": "sTGP2-KCrVfw"
      },
      "id": "sTGP2-KCrVfw"
    },
    {
      "cell_type": "code",
      "source": [
        "pre_prompt = \"\"\"\n",
        "Tu es un ChatBot spécialisé en fromages.\n",
        "L'utilisateur est un amateur de fromages de longue date et il connait lui aussi de nombreux fromages.\n",
        "Ton objectif est de lui faire découvrir des fromages peu connu et insolites en fonction de ses demandes.\n",
        "Acceuille le avec un message de bienvenu et en te presentant.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": pre_prompt\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "aWINiqMWrpMS"
      },
      "id": "aWINiqMWrpMS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, nous allons définir les paramètres généraux à utiliser lors de la génération de textes par le ChatBot."
      ],
      "metadata": {
        "id": "6H2MIQGesKPH"
      },
      "id": "6H2MIQGesKPH"
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "    # pour utiliser le paramètre \"temperature\", passer le paramètre \"do_sample\" à True\n",
        "    #\"temperature\": float(SEED)\n",
        "}"
      ],
      "metadata": {
        "id": "277PuNHbsW1s"
      },
      "id": "277PuNHbsW1s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et enfin nous pouvons lancer une boucle qui génére un message du ChatBot, attends un message de l’utilisateur et complète automatiquement la conversation.\n",
        "\n",
        "La conversation s’arrête lorsque l’utilisateur saisi le mot \"fin\"."
      ],
      "metadata": {
        "id": "dAYl5Rfgsczj"
      },
      "id": "dAYl5Rfgsczj"
    },
    {
      "cell_type": "code",
      "source": [
        "usr_input = \"\"\n",
        "while usr_input != \"fin\":\n",
        "  # On genere un message du ChatBot\n",
        "  output = text_generator(messages, **generation_args)\n",
        "\n",
        "  # On affiche le message\n",
        "  print(\"\\nChatBot:\", output[0]['generated_text'], \"\\n\")\n",
        "\n",
        "  # On attend l'entree de l'utilisateur\n",
        "  usr_input = input(\"Utilisateur:\")\n",
        "\n",
        "  # On met à jour la conversation\n",
        "  messages.append({\"role\": \"assistant\", \"content\": output[0]['generated_text']})\n",
        "  messages.append({\"role\": \"user\", \"content\": usr_input})\n"
      ],
      "metadata": {
        "id": "k-x36KUr1IuW"
      },
      "id": "k-x36KUr1IuW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Techniques d’instructions (Prompt Engineering)"
      ],
      "metadata": {
        "id": "b9YCzP_FrQJM"
      },
      "id": "b9YCzP_FrQJM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant de clôre cette partie, nous allons aborder différentes techniques, issu du domaine du *Prompt Engineering*, permettant d’optimiser et maximiser la qualité des réponses fournie par un agent conversationnel.\n",
        "\n",
        "Parmis ces techniques, une très simple a employé est celle de la chaîne de pensée, ou *Chain of Thoughts* (CoT) en anglais.\n",
        "\n",
        "Pour utiliser cette technique, il suffit d’ajouter \"Pensons étape par étape\" (*Let's think step by step*) à notre requête.\n",
        "\n",
        "Cela forcera la probabilité que le ChatBot génère un texte avec plusieurs étapes, donc un contenu plus détaillé et donc une réponse de meilleure qualité.\n",
        "\n",
        "Pour en savoir plus, voir: https://www.promptingguide.ai/techniques/cot"
      ],
      "metadata": {
        "id": "mXXDiTXHra-p"
      },
      "id": "mXXDiTXHra-p"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Pensons étape par étape.\n",
        "        \"\"\" + query\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "vgIFgepu8ZwT"
      },
      "id": "vgIFgepu8ZwT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "QvPm8r3E9Stk"
      },
      "id": "QvPm8r3E9Stk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cependant, la plupart des llm sont maintenant entrainé pour utiliser automatiquement du CoT lors de la génération de leurs réponses.\n",
        "\n",
        "Une autre technique possible, dérivée de CoT, est l’arbre de pensées, ou *Tree of Thoughts* (ToT) en anglais.\n",
        "\n",
        "Cette technique reprends le principe de CoT, mais détaillant étape par étape des réponses selon différents point de vue et en comparant ces réponses entre elles.\n",
        "\n",
        "Pour en savoir plus, voir: https://www.promptingguide.ai/techniques/tot"
      ],
      "metadata": {
        "id": "Sh75-Bl-9dM_"
      },
      "id": "Sh75-Bl-9dM_"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Imagine que trois experts différents débattent pour répondre à cette question.\n",
        "          Chaque expert écrit une étape de sa réflexion et la partage avec le groupe.\n",
        "          Puis tous les experts continue ainsi de suite, étape par étape.\n",
        "          Si un expert réalise qu'il s'est trompé à un moment de sa réflexion, il sort de la discussion.\n",
        "          La question est:\n",
        "        \"\"\" + query\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "csa3lmHv9cum"
      },
      "id": "csa3lmHv9cum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args[\"max_new_tokens\"] = 5000"
      ],
      "metadata": {
        "id": "QcKWzGIwzXQQ"
      },
      "id": "QcKWzGIwzXQQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "7c-1UNDg-rj-"
      },
      "id": "7c-1UNDg-rj-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfin, il est aussi possible de demander au ChatBot de générer des connaissances générales en lien avec la question posée.\n",
        "\n",
        "Cette technique, appellée *Generated Knowledge Prompting*, guide alors la génération de texte à l’aide de ces connaissances fourni explicitement.\n",
        "\n",
        "Pour en savoir plus, voir: https://www.promptingguide.ai/techniques/knowledge"
      ],
      "metadata": {
        "id": "7oOdIABF_xHJ"
      },
      "id": "7oOdIABF_xHJ"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Avant de répondre à la question, écrit les différentes connaissances que tu as sur le sujet.\n",
        "          La question est:\n",
        "        \"\"\" + query\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "o4LAwdWA_lf6"
      },
      "id": "o4LAwdWA_lf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "rBus3JEAAEpr"
      },
      "id": "rBus3JEAAEpr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien entendu, ces différentes techniques ne sont pas mutuellement exclusives et pouvent se combiner entre elles.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "Io1UP0Okzly7"
      },
      "id": "Io1UP0Okzly7"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "        \"\"\"\n",
        "          Imagine que trois experts différents débattent pour répondre à cette question.\n",
        "          Chaque expert a des connaissances différentes des autres sur le sujet.\n",
        "          Avant de débattre, chaque expert énonce ses connaissances sur le sujet.\n",
        "          Ensuite, chaque expert écrit une étape de sa réflexion et la partage avec le groupe.\n",
        "          Chaque expert peut réfuter les arguments d'un autre expert.\n",
        "          Si un expert réalise qu'il s'est trompé à un moment de sa réflexion, il sort de la discussion.\n",
        "          Ainsi de suite jusqu'à arriver à un consensus apportant une réponse à la question.\n",
        "          La question est: \"Je cherche un fromage de Savoie, que me conseille-tu?\"\n",
        "        \"\"\"\n",
        "     }\n",
        "]"
      ],
      "metadata": {
        "id": "ER2J7N-XAZAN"
      },
      "id": "ER2J7N-XAZAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = text_generator(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "IFjwCI3nAmWb"
      },
      "id": "IFjwCI3nAmWb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De nombreuses autres techniques de *Prompt Engineering* existent, telles que [Self-Consistency](https://www.promptingguide.ai/techniques/consistency) ou [ReAct](https://www.promptingguide.ai/techniques/react), et bien d’autres vont surement voir le jour dans les prochaines années.\n",
        "\n",
        "L’objectif étant alors de trouver les bonnes formulations pour répondre le plus efficacement à telle ou telle demande. Des techniques comme l’[APE](https://www.promptingguide.ai/techniques/ape) pour *Automatic Prompt Engineer* propose même de générer ces formulations via un modèle de langage.\n",
        "\n",
        "Pour aller plus loin sur le sujet, voir: https://www.promptingguide.ai/"
      ],
      "metadata": {
        "id": "MDd4Y5ySzzUn"
      },
      "id": "MDd4Y5ySzzUn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partie 3 - Combiner encodage et décodage avec les LLM Agents"
      ],
      "metadata": {
        "id": "zbtuxkqyrcIS"
      },
      "id": "zbtuxkqyrcIS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour conclure cet atelier, nous allons voir dans cette partie comment créer un assistant personnel minimal en combinant les différentes techniques et méthodes vues dans les partie précédentes."
      ],
      "metadata": {
        "id": "hLBVRBdirrgW"
      },
      "id": "hLBVRBdirrgW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Generation augmentée par requêtes (RAG)"
      ],
      "metadata": {
        "id": "4XeHN8xiXgb2"
      },
      "id": "4XeHN8xiXgb2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La géneration augmentée par requêtes, ou *Retrieval Augmented Generation* (RAG) en anglais, est une technique de *Prompt Engineering* consistant à compléter une question avec des informations vérifiées et stockées en base.\n",
        "\n",
        "Nous pouvons, par exemple, réutiliser la fonction créée en partie 1 afin de retrouver des documents correspondants à notre requête."
      ],
      "metadata": {
        "id": "pRymFLkV9mk-"
      },
      "id": "pRymFLkV9mk-"
    },
    {
      "cell_type": "code",
      "source": [
        "docs = find_embedded_texts(query)\n",
        "docs"
      ],
      "metadata": {
        "id": "Zx34j7JTY-ex"
      },
      "id": "Zx34j7JTY-ex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, il nous faut créer une nouvelle requête complétant la première avec les informations retrouvées et indiquant à notre llm d’utiliser ses informations."
      ],
      "metadata": {
        "id": "yAiba7nM_OQz"
      },
      "id": "yAiba7nM_OQz"
    },
    {
      "cell_type": "code",
      "source": [
        "usr_input = \"Pour répondre à la question base toi sur les informations suivantes, en indiquant les informations utilisées et comment:\"\n",
        "\n",
        "for i, doc in enumerate(docs):\n",
        "  usr_input += \"\\n\\nInformation \" + str(i) + \": \\\"\" + doc[\"doc\"] + \"\\\"\"\n",
        "\n",
        "usr_input += \"\\n\\nla question est: \\\"\" + query + \"\\\"\"\n",
        "\n",
        "print(usr_input)"
      ],
      "metadata": {
        "id": "skIaKP5RZ6O6"
      },
      "id": "skIaKP5RZ6O6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons alors fournir cette nouvelle requête à notre llm, qui devrait générer une réponse basée sur les informations que nous avons retrouvées en base."
      ],
      "metadata": {
        "id": "BjzxvwoA_3Cv"
      },
      "id": "BjzxvwoA_3Cv"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": usr_input\n",
        "     }\n",
        "]\n",
        "\n",
        "output = text_generator(messages, return_full_text = False, max_new_tokens=500)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "W9Vao1uKbAkj"
      },
      "id": "W9Vao1uKbAkj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour en savoir plus sur le RAG, voir: https://www.promptingguide.ai/techniques/rag"
      ],
      "metadata": {
        "id": "FB5qDPu19-ac"
      },
      "id": "FB5qDPu19-ac"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Utilisation d’outils"
      ],
      "metadata": {
        "id": "UvzBKrurXuKE"
      },
      "id": "UvzBKrurXuKE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est aussi possible d’indiquer au llm les fonctions, aussi appellé outils ou *tools*, qu’il peut utiliser pour répondre aux requêtes d’un utilisateur, cela afin d’automatiser leur utilisation.\n",
        "\n",
        "Pour cela, il nous faut lui fournir un *prompt* avec les instructions nécessaires.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "sw3FyxM9AO9c"
      },
      "id": "sw3FyxM9AO9c"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "          \"\"\"\n",
        "            Pour répondre à la question, utilise la fonction \"find_embedded_texts\" et base toi sur ses résultats en indiquant quelle information tu as utilisé et comment.\n",
        "\n",
        "            Pour utiliser cette fonction, écrit uniquement et simplement son nom et ses arguments, comme suit:\n",
        "\n",
        "            {\"name\": \"find_embedded_texts\", \"args\": {\"query\": \"question de l'utilisateur\"}}\n",
        "\n",
        "            La question est:\n",
        "          \"\"\" + query\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "44-cUzIZX8m3"
      },
      "id": "44-cUzIZX8m3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce *prompt* permet d’aiguiller la génération de texte afin que le llm écrive la fonction à appeler."
      ],
      "metadata": {
        "id": "jluiC5lTA7qD"
      },
      "id": "jluiC5lTA7qD"
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = text_generator(messages, return_full_text = False, max_new_tokens=500)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "ExDmZL861acN"
      },
      "id": "ExDmZL861acN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il nous faut alors récupérer les éléments pour appeler la fonction."
      ],
      "metadata": {
        "id": "9ErWe9ofBQNY"
      },
      "id": "9ErWe9ofBQNY"
    },
    {
      "cell_type": "code",
      "source": [
        "function_called = json.loads(re.findall(r'\\{\\\"name\\\": \\\"find_embedded_texts\\\", \\\"args\\\": \\{.+\\}\\}', generated_text)[0])\n",
        "function_called"
      ],
      "metadata": {
        "id": "EKLHTJhTE0qg"
      },
      "id": "EKLHTJhTE0qg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour ensuite appeler la fonction avec les arguments générés par le llm."
      ],
      "metadata": {
        "id": "OY36gbpQB_sw"
      },
      "id": "OY36gbpQB_sw"
    },
    {
      "cell_type": "code",
      "source": [
        "docs = find_embedded_texts(function_called[\"args\"][\"query\"])\n",
        "docs"
      ],
      "metadata": {
        "id": "rUXAESoqHJk2"
      },
      "id": "rUXAESoqHJk2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis compléter la discussion avec les résultats de la recherche."
      ],
      "metadata": {
        "id": "NjvTDnARCHcE"
      },
      "id": "NjvTDnARCHcE"
    },
    {
      "cell_type": "code",
      "source": [
        "system_text = \"Résultats de la fonction:\"\n",
        "\n",
        "for i, doc in enumerate(docs):\n",
        "  system_text += \"\\n\\nInformation \" + str(i) + \": \\\"\" + doc[\"doc\"] + \"\\\"\"\n",
        "\n",
        "messages.append({\"role\": \"assistant\", \"content\": generated_text})\n",
        "messages.append({\"role\": \"user\", \"content\": system_text})\n",
        "\n",
        "messages"
      ],
      "metadata": {
        "id": "sjasbiRvHkNm"
      },
      "id": "sjasbiRvHkNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour enfin demander au llm de nous fournir une réponse basées sur les résultats retrouvé."
      ],
      "metadata": {
        "id": "QvVB-5fGCPgA"
      },
      "id": "QvVB-5fGCPgA"
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = text_generator(messages, return_full_text = False, max_new_tokens=500)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "VrE3lq_OIj-V"
      },
      "id": "VrE3lq_OIj-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l’exemple précédent, nous avons demandé au llm de rechercher des informations correspondants à une requête.\n",
        "\n",
        "Mais, de manière similaire, il est tout à fait possible de lui demander d’enregistrer de nouvelles informations.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "NrBqZ8YGCxxh"
      },
      "id": "NrBqZ8YGCxxh"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "          \"\"\"\n",
        "            Pour enregistrer des informations, utilise la fonction \"store_text_embeddings\".\n",
        "\n",
        "            Pour utiliser cette fonction, écrit simplement son nom et ses arguments, comme suit:\n",
        "\n",
        "            {\"name\": \"store_text_embeddings\", \"args\": {\"text\": \"information à enregistrer\"}}\n",
        "\n",
        "            Enregistre l'information suivante: \"La mozzarella ou mozzarelle est un fromage à pâte filée traditionnel de la cuisine italienne, à base de lait de vache ou de bufflonne. Deux de ces fromages bénéficient d'une appellation d'origine protégée, la mozzarella di bufala campana produite en Campanie avec du lait de bufflonne, et la mozzarella di Gioia del Colle produite dans les Pouilles avec du lait de vache\"\n",
        "          \"\"\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "hgTmZzPehWbj"
      },
      "id": "hgTmZzPehWbj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = text_generator(messages, return_full_text = False, max_new_tokens=500)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "rV_KNuw_1b9q"
      },
      "id": "rV_KNuw_1b9q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function_called = json.loads(re.findall(r'\\{\\\"name\\\": \\\"store_text_embeddings\\\", \\\"args\\\": \\{.+\\}\\}', generated_text)[0])\n",
        "store_text_embeddings(function_called[\"args\"][\"text\"])"
      ],
      "metadata": {
        "id": "FukJonTuap9s"
      },
      "id": "FukJonTuap9s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_texts"
      ],
      "metadata": {
        "id": "0RvusAZJbD3U"
      },
      "id": "0RvusAZJbD3U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_embeddings"
      ],
      "metadata": {
        "id": "bG2dy-krbUMr"
      },
      "id": "bG2dy-krbUMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons demander à notre llm de générer des fiches thématiques à partir d’une information données et de faire les appels correspondants.\n",
        "\n",
        "Par exemple:"
      ],
      "metadata": {
        "id": "0RVvgQ1wDsmJ"
      },
      "id": "0RVvgQ1wDsmJ"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "          \"\"\"\n",
        "            Pour enregistrer des informations, utilise la fonction \"store_text_embeddings\".\n",
        "\n",
        "            Pour utiliser cette fonction, écrit simplement son nom et ses arguments, comme suit:\n",
        "\n",
        "            {\"name\": \"store_text_embeddings\", \"args\": {\"text\": \"information à enregistrer\"}}\n",
        "\n",
        "            Génère et enregistre 5 fiches, et seulement 5, de l'information suivante, chaque fiche portant sur un élément particulier:\n",
        "\n",
        "            \"Le roquefort ([ʁɔk(ə)fɔːʁ]), ou ròcafòrt en occitan rouergat, est un fromage à pâte persillée élaboré dans le sud de la France exclusivement avec des laits crus de brebis.\n",
        "            La meilleure période de consommation de ce fromage français s'étend de janvier à août.\n",
        "            Ce fromage est mentionné expressément pour la première fois au xie siècle2, ce qui en fait un symbole historique de la région des causses et vallées de l'Aveyron. Cette région rurale, établie sur un terroir parfois très difficile à exploiter, en a fait sa richesse financière et culturelle.\n",
        "            De réputation internationale, il est associé à l'excellence de l'agriculture française et à sa gastronomie. Il est même devenu l'emblème de la résistance des producteurs et transformateurs de fromage au lait cru contre les demandes réitérées de la généralisation de la pasteurisation du lait[réf. souhaitée]. Il existe aujourd'hui sous forme industrielle et laitière, sa forme fermière subsiste toutefois encore mais marginalement.\n",
        "            En 1925, en France, « roquefort » est la première appellation d'origine dont l'emploi, du moins dans sa désignation d'un fromage voué au commerce, est encadré administrativement.\n",
        "            Depuis 1979, cette appellation bénéficie d'une appellation d'origine contrôlée (AOC) et, depuis 1996, d'une appellation d'origine protégée (AOP)3. Cette AOP se caractérise par les grottes afférentes aux fleurines, des failles dans la roche créant un flux d'air continu et donnant une humidité de 80 % minimum et une température de 10 °C en moyenne. Produit par sept fabricants4 sur une zone de 2 000 × 300 m, le fromage s'affine. Ces conditions permettent le développement du Penicillium roqueforti qui donne au fromage son goût unique.\n",
        "            La marque Société du groupe Lactalis produit 70% du roquefort en France et domine donc largement le secteur de production de ce fromage5.\"\n",
        "          \"\"\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "2JZXApmyjSoG"
      },
      "id": "2JZXApmyjSoG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = text_generator(messages, return_full_text = False, max_new_tokens=500)[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "F_2j5fXu1d2f"
      },
      "id": "F_2j5fXu1d2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_calls = re.findall(r'\\{\\\"name\\\": \\\"store_text_embeddings\\\", \\\"args\\\": \\{.+\\}\\}', generated_text)\n",
        "find_calls"
      ],
      "metadata": {
        "id": "ktvAxBihcJkw"
      },
      "id": "ktvAxBihcJkw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for call in find_calls:\n",
        "  function_called = json.loads(call)\n",
        "  store_text_embeddings(function_called[\"args\"][\"text\"])"
      ],
      "metadata": {
        "id": "feaE7LvEcVhf"
      },
      "id": "feaE7LvEcVhf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_texts"
      ],
      "metadata": {
        "id": "2Lj8bV7IcoLb"
      },
      "id": "2Lj8bV7IcoLb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bdd_embeddings"
      ],
      "metadata": {
        "id": "Gcw2dVJNcp-H"
      },
      "id": "Gcw2dVJNcp-H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour plus d’information sur l’utilisation d’outils par les llm, voir: https://python.langchain.com/docs/use_cases/tool_use/"
      ],
      "metadata": {
        "id": "h9yF8fS1cfeo"
      },
      "id": "h9yF8fS1cfeo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Création d’un agent\n"
      ],
      "metadata": {
        "id": "vQo35bw9X9EQ"
      },
      "id": "vQo35bw9X9EQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons maintenant tous les éléments pour créer notre propre agents LLM.\n",
        "\n",
        "Tout d’abord, nous allons créer un *pre-prompt* pour l’initialiser avec les instructions necessaires à son fonctionnement."
      ],
      "metadata": {
        "id": "niGuvX0aNKqU"
      },
      "id": "niGuvX0aNKqU"
    },
    {
      "cell_type": "code",
      "source": [
        "pre_prompt = \"\"\"\n",
        "Tu es un ChatBot spécialisé en fromages.\n",
        "\n",
        "Tu as à ta disposition les fonctions suivantes:\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"name\": \"find_embedded_texts\",\n",
        "    \"args\": [{\n",
        "      \"name\": \"query\",\n",
        "      \"type\": str\n",
        "      }],\n",
        "    \"descr\": \"recherche en base de données des documents en lien avec la demande de l'utilisateur.\"\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"store_text_embeddings\",\n",
        "    \"args\": [{\n",
        "      \"name\": \"text\",\n",
        "      \"type\": str\n",
        "      }],\n",
        "    \"descr\": \"enregistre un texte en base de données pour pouvoir le rechercher plus tard\"\n",
        "  }\n",
        "]\n",
        "\n",
        "Pour utiliser une de ces fonctions, écrit simplement son nom et ses arguments, comme suit:\n",
        "\n",
        "{\"name\": \"store_text_embeddings\", \"args\": {\"text\": \"information à enregistrer\"}}\n",
        "\n",
        "{\"name\": \"find_embedded_texts\", \"args\": {\"query\": \"question de l'utilisateur\"}}\n",
        "\n",
        "C'est à toi de le faire, pas à l'utilisateur.\n",
        "\n",
        "Ne mentionne ces fonctions que pour les utiliser, pas pour les expliquer à l'utilisateur.\n",
        "\n",
        "L'utilisateur est un amateur de fromages de longue date et il connait lui aussi de nombreux fromages.\n",
        "\n",
        "Ton objectif est de lui faire découvrir des fromages peu connu et insolites en fonction de ses demandes.\n",
        "\n",
        "Réponds à ses demandes en utilisant les fonctions à ta disposition, en détaillant tes réponses étape par étape et en indiquant quelles informations tu as utilisé et comment.\n",
        "\n",
        "Acceuille le avec un message de bienvenu et en te presentant.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KMy8qBO_Npby"
      },
      "id": "KMy8qBO_Npby",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il nous faut aussi créer une fonction qui va nous permettre de détecter les fonctions à appeler."
      ],
      "metadata": {
        "id": "1GTUHaD4PaXz"
      },
      "id": "1GTUHaD4PaXz"
    },
    {
      "cell_type": "code",
      "source": [
        "def call_functions(generated_text):\n",
        "  find_calls = re.findall(r'\\{\\\"name\\\": \\\".+\\\", \\\"args\\\": \\{.+\\}\\}', generated_text)\n",
        "\n",
        "  results = \"\"\n",
        "  for call in find_calls:\n",
        "    function_called = json.loads(call)\n",
        "    if function_called[\"name\"] == \"store_text_embeddings\":\n",
        "      store_text_embeddings(function_called[\"args\"][\"text\"])\n",
        "      results += \"\\nRésultat de la fonction: document enregistré\\n\"\n",
        "    elif function_called[\"name\"] == \"find_embedded_texts\":\n",
        "      docs = find_embedded_texts(function_called[\"args\"][\"query\"])\n",
        "      results += \"\\nRésultat de la fonction: \" + str(docs) + \"\\n\"\n",
        "  return results"
      ],
      "metadata": {
        "id": "TUhEqlD0c0Y-"
      },
      "id": "TUhEqlD0c0Y-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et enfin, nous pouvons lancer notre agent LLM."
      ],
      "metadata": {
        "id": "O70sbKT2fesb"
      },
      "id": "O70sbKT2fesb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du pre-prompt\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": pre_prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# Initialisation des paramètres\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "    # pour utiliser le paramètre \"temperature\", passer le paramètre \"do_sample\" à True\n",
        "    #\"temperature\": float(SEED)\n",
        "}\n",
        "\n",
        "# Lancement de l'agent\n",
        "usr_input = \"\"\n",
        "while usr_input != \"fin\":\n",
        "  # On recupere le texte genere par l'agent\n",
        "  output = text_generator(messages, **generation_args)\n",
        "  messages.append({\"role\": \"assistant\", \"content\": output[0]['generated_text']})\n",
        "\n",
        "  # On affiche le résultat\n",
        "  print(\"\\nChatBot:\", output[0]['generated_text'], \"\\n\")\n",
        "\n",
        "  # On appelle les fonctions si besoin\n",
        "  usr_input = call_functions(output[0][\"generated_text\"])\n",
        "\n",
        "  # Si aucune fonction n'a ete appele on recupere l'entree de l'utilisateur\n",
        "  if usr_input != \"\":\n",
        "    print(\"\\nSystem:\", usr_input, \"\\n\")\n",
        "  else:\n",
        "    usr_input = input(\"Utilisateur:\")\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": usr_input})"
      ],
      "metadata": {
        "id": "QOU2MvSuYMy_"
      },
      "id": "QOU2MvSuYMy_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Pour des raisons pédagogiques, nous avons coder nous même les fonctionnalités de cette agent. Il existe cependant des librairies telles que [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) qui vous permetteront de développer plus efficacement un agent LLM.\n",
        "\n",
        "Vous connaissez maintenant les principes de base vous permettant de construire un assistant personnel basé sur un LLM.\n",
        "\n",
        "N’hésitez pas à modifier ce notebook selon vos envies et vos besoins 😀\n",
        "\n",
        "Et si ce notebook vous a été utile, n’oubliez pas qu’il est sous licence [BeerWare](https://fr.wikipedia.org/wiki/Beerware) 😏"
      ],
      "metadata": {
        "id": "zxfvE6u7DdD6"
      },
      "id": "zxfvE6u7DdD6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}